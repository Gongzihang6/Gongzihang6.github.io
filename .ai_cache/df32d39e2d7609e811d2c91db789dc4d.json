{
  "summary": "批量梯度下降(BGD)通过全数据集计算精确梯度，收敛稳定但计算成本高；随机梯度下降(SGD)每次随机选取单个样本更新参数，速度快但方向不稳定；小批量梯度下降(MBGD)折中二者优势，采用小批量样本实现计算效率与稳定性的平衡。学习率选择需适中，过大导致震荡，过小影响收敛速度。这三种优化算法在机器学习模型训练中各具特点，需根据数据规模和计算资源灵活选用。",
  "service": "deepseek",
  "page_title": "机器学习之最优化算法",
  "timestamp": "2026-01-02T07:56:05.662155",
  "language": "zh"
}