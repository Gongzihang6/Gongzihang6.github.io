[TOC]



# K 近邻学习与降维

## K 近邻

### 基本思想

**近朱者赤，近墨者黑；** k 近邻（k-Nearnes Neighbor，简称 kNN）学习是一种常用的<span style="color:#3490de;">监督学习</span>方法；

工作机制：给定测试样本，基于某种 **距离度量**，找出训练集中与其最靠近的 k 个训练样本，然后基于这 k 个“邻居”的信息来进行预测；

![kNN 工作机制解释图](https://cdn.jsdelivr.net/gh/Gongzihang6/Pictures@main/Medias/image-20250613153215328.png)

#### kNN 实现步骤

-   确定训练样本，以及某种距离度量；

-   对于某个给定的测试样本，找到训练集中距离最近的 k 个样本

    -   对于分类为题使用“投票法”获得预测结果；
    -   对于回归问题使用“平均法”获得预测结果；
    -   也可基于距离远近进行<span style="color:#d59bf6;">**加权平均或加权投票**</span>

-   投票法：选择这 k 个样本中出现最多的类别标记作为预测结果；

-   平均法：将这 k 个样本的标签的平均值作为测试样本的预测结果；

    >   **缺点是计算开销比较大，对于每个待测试样本，都要计算其与整个训练集中所有样本的距离，特别是维度比较高时计算开销更大**

两种特殊的数据结构提前对训练集进行优化存储，提高对新样本预测的效率；

---

##### Kd-Tree

-   <span style="color:#d59bf6;">Kd-Tree，kd树是一种对k维特征空间中的实例点进行存储以便对其快速检索的树形结构；</span>

kd 树可以理解为 k 叉树，核心思想是对 k 维特征空间中的样本点进行不断划分，来构造树结构。具体构造过程如下：1、对整个样本空间，计算所有样本点每一维坐标的方差，每次选择维度坐标方差最大的维度进行划分，具体划分点选择该维坐标的中位数（选择方差最大的坐标维度，可以最大程度的区分开左右子树节点内的样本点）；2、重复步骤 1，选择方差第二大的坐标维度的中位数进行划分，直到左右子树中只有一个样本点，也就是划分到叶子节点；

kd 树构造完成后，给出一个测试样本 $t$，如何在训练集中检索距离测试样本最近的 k 个训练样本呢？具体过程如下：

1、从根节点出发，递归地向下访问 kd 树，如果测试样本当前维度的坐标值小于切分值（也就是所有训练样本中该维坐标的中位数），则向下访问左子树，否则向下访问右子树，直至到达叶节点 $x$；以该叶节点为当前测试样本的“最近点”；

2、记录测试样本 $t$ 到当前“最近点”$x$ 的距离 $d$，如果该距离 $d$ 比“最近点”$x$ 到父节点 $f(x)$ 的距离小，则说明 $x$ 的兄弟节点中不存在到测试样本 $t$ 距离更近的样本点，可以直接跳过搜索（否则不能跳过，必须一一计算测试样本 $t$ 到兄弟节点中所有样本点的距离，看是否有距离比 $d$ 小的，如果有就更新“最近点”），此时回退到父节点 $f(x)$ 的兄弟节点树，通过判断 $d$ 和 **测试样本到父节点的兄弟节点树的根节点的距离** 的大小，如果 $d$ 更小，则该兄弟节点树也可以跳过(否则必须一一计算距离判断和 $d$ 的大小关系)，然后继续判断 d 和父节点的父节点的距离大小关系，来决定是否剪枝；

```
1、从根结点出发，递归地向下访问kd树。若预测样本 S 的坐标在当前维小于切分点，则向左访问，反之，向右访问，直到访问到叶节点，取该叶节点作为当前 S 的最近邻点。
2、以 S 和当前最近邻点做圆。
3、从当前叶节点回退父节点，并访问父节点的另一个分支，检查该节点或者子空间是否与圆相交
4、若相交，则需要往下访问子节点，查找是否存在比当前最近邻点更近的节点
5、若存在更近的样本，则更新最近邻点，并重复步骤2
6、若不存在，则重复步骤3
7、若不相交，则不需要访问这一分支，继续回退到上一层父节点，重复步骤3
8、退回到根节点后结束搜索过程，并获得最近邻点。
```

这里“检查该节点或者子空间是否与圆相交”的方法就是判断 $d$ 和 **当前“最近点”到分割点的距离** 的大小关系，如果 $d$ 小，则说明不相交，可以跳过，提高搜索效率；否则不能跳过；

以上步骤实现了搜索距离测试样本 **最近点**，如何实现前 k 个最近点的搜索呢？用最大堆实现。用前面的算法从上至下搜索到的第一个叶子节点作为最大堆的第一个节点，在往上回退的过程当中：

-   如果出现不能跳过的区域，将测试样本点和该区域中的训练样本点计算出的距离，逐步向大根堆中添加，如果大根堆中节点数量还不足 k 个，则直接增加一个节点并排序，如果已经有 k 个，则对比该距离值和大根堆中根节点值，如果大于根节点值，则不改变大根堆，如果小于根节点值，则替换大根堆根节点并重新排序；

这样回退到 Kd-Tree 的根节点后，大根堆中维护的就是距离测试样本最近的 k 个训练样本，就可以通过这 k 个训练样本预测测试样本的标签；

>   kd 树在维度小于 20 时效率最高，一般适用于训练实例数远大于空间维数时的 k 近邻搜索，当空间维数接近训练实例数时，它的效率会迅速下降，几乎接近线性扫描

##### Kd-Ball

-   Kd-Ball，为了解决 kd 树在样本特征维度很高时效率低下的问题，研究人员提出了“球树“（BallTree）。KD 树沿坐标轴分割数据，**BallTree 则在一系列嵌套的超球面上分割数据，即使用超球面而不是超矩形划分区域。**

Ball-Tree 构建过程如下：

1.  **构建根节点球体**：首先，计算一个能够包含所有训练样本的最小超球面，作为根节点。
2.  **选择分裂点**：从球中选择一个离球心最远的点，再选择一个离该点最远的点，这两个点可以作为初始的两个簇心。
3.  **划分数据**：将球内的所有点分配给离它最近的簇心，形成两个子集。
4.  **生成子球体**：为这两个子集分别计算最小的包围超球面，形成两个子节点。
5.  **递归构建**：对这两个子球体递归地执行以上划分过程，直到叶子节点包含的样本数小于某个阈值。

优化 kNN 的搜索过程：

1.  向下搜索：从根节点（最大的球）开始，判断查询点距离左子球球心更近还是距离右子球球心更近，选择更近的子球向下搜索；
2.  更新最近邻：向下搜索，直到叶子节点，以该叶子节点为“最近点”，然后和 kd 树一样，向上回退，维护一个大小为 k 的大根堆，直到回退到根节点；
3.  剪枝：在向上回退过程中，在决定是否要进入一个子球体进行搜索之前，线计算查询样本点到该球体边界的最小距离
    -   如果这个最小距离已经大于大根堆中根节点值，那么这个球体内不可能存在更近的邻居，可以直接跳过；
    -   否则，计算查询样本点和该子球体中所有样本的距离，和大根堆根节点值进行判断，尝试更新大根堆；
4.  完成搜索：回退到根节点时，搜索结束，大根堆中维护的 k 个节点就是所求结果；

---

### kNN 的特点

-   是一种基于实例的学习，需要一个邻近性度量来确定实例间的相似性或距离；
-   不需要建立模型，但分类一个测试样例开销很大（尤其是样本特征维度很大、实例很多的时候），需要计算测试样例和所有训练实例之间的距离；
-   <span style="background:#6fe7dd; border-radius:5px; display:inline-block;">基于局部信息进行预测，对噪声非常敏感；</span>
-   kNN 分类器可以生成 **任意形状** 的决策边界；
    -   决策树和基于规则的分类器通常是直线决策边界；
-   需要适当的邻近性度量和数据预处理；如数据标准化等，防止邻近性度量被某个属性主导；

急切学习（Eager Learner）

-   使用特定模型，对训练样本进行训练和学习，从中发现数据规律/模式，训练开销大；推理相对开销小；

-   两步过程：（1）归纳	（2）演绎

惰性学习（Lazy Learner）

-   此类学习技术在训练阶段仅仅是把样本保存起来，训练时间开销为零，待收到测试样本后再进行处理。
-   Rote-learner（死记硬背）
    -   记住所有的训练数据，仅当测试样本的属性值与一个训练样本完全匹配才对它分类；
-   最近邻（Nearest Neighbor）
    -   使用最近的 k 个点（最近邻）进行分类；

### 1-NN 二分类错误率

暂且假设距离计算是“恰当的”，即能够恰当的找出 k 个近邻，对“最近邻分类器”（1-NN）在二分类问题上的性能进行讨论；

给定测试样本 $x$，若其最近邻样本为 $z$，则最近邻出错的概率就是 $x$ 与 $z$ 类别标记不同的概率，即
$$
P(err)= 1-\sum_{c\in y}P(c|x)P(c|z)
$$
其中 $y=\{1,-1\}$，$\sum\limits_{c\in y}P(c|x)P(c|z)$ 表示测试样本 $x$ 和最近邻样本 $z$ 属于同一类的概率，由于没有指定具体的类别 c，因此需要对所有类别求和，即考虑“属于任意相同类”的总概率；

假设样本独立同分布，且对任意 $x$ 和任意小整数$\delta$，在 $x$ 附近$\delta$ 距离范围内总能找到一个训练样本（也就是训练样本足够密集）；换言之，对任意训练样本，总能在任意近的范围内找到一个训练样本 z，使得对该测试样本预测错误的概率为 $P(err)= 1-\sum\limits_{c\in y}P(c|x)P(c|z)$

令 $c^*=\mathop{\arg\max}\limits_{c\in y}P(c|x)$ 表示贝叶斯最优分类器的结果，也就是给定测试样本，它属于 $c^*$ 类的概率最大；有


$$
\begin{aligned} 
P(err)&= 1-\sum\limits_{c\in y}P(c|x)P(c|z) \approx 1-\sum\limits_{c\in y}P^2(c|x)	\\
&\leq 1-P^2(c^*|x)=(1+P(c^*|x))(1-P(c^*|x))	\\
&\leq 2\times (1-P(c^*|x)). 
\end{aligned}
$$



第一行的约等于是因为 $x$ 和 $z$ 是可以任意近的，因为它们之间的距离小于任意正数$\delta$，所以它们属于任意同一类别 $c$ 的概率可以认为很接近（这也是 k 近邻的核心思想）；$c^*$ 是测试样本 $x$ 最可能属于的类别，因此 $\sum\limits_{c\in y}P(c|x)>=P(c^*|x)$

公式（2）说明，最近邻分类虽然简单，但在距离度量合适、训练样本足够密集的假设下，它的泛化错误率不超过贝叶斯最优分类器的两倍；

### 维数灾难

上述讨论基于一个重要的假设：任意测试样本 $x$ 附近的任意小的 $\delta$ 距离范围内总能找到一个训练样本，即训练样本的采样密度足够大，或称为“密采样”。然而，这个假设在现实任务中通常很难满足:

-   若属性维数为 1，当 $\delta=0.001$，仅考虑单个属性，则仅需 1000 个样本点平均分布在归一化后的属性取值范围内，即可使得任意测试样本在其附近 0.001 距离范围内总能找到一个训练样本，此时最近邻分类器的错误率不超过贝叶斯最优分类器的错误率的两倍。若属性维数为 20，若样本满足密采样条件，则至少需要 $(10^3)^{20}=10^{60}$ 个样本。
-   **现实应用中属性维数经常成千上万，要满足密采样条件所需的样本数**
    **目是无法达到的天文数字。** 许多学习方法都涉及距离计算，而高维空
    间会给距离计算带来很大的麻烦，例如当维数很高时甚至连计算内积
    都不再容易。
-   <span style="color:#d59bf6;">在高维情形下出现的数据样本稀疏、距离计算困难等问题，是所有机</span>
    <span style="color:#d59bf6;">器学习方法共同面临的严重障碍，被称为“维数灾难”。</span>

缓解维数灾难的一个重要途径是降维（dimension reduction）

-   即通过某种数学变换，将原始高位属性空间转变为一个低维“子空间”（subspace）在这个子空间中样本密度大幅度提高，距离计算也变得更为容易；

为什么能进行降维？

-   数据样本虽然是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间中的一个 **低维嵌入 （embedding )**，因而可以对数据进行有效的降维。

![低维嵌入示意图](https://cdn.jsdelivr.net/gh/Gongzihang6/Pictures@main/Medias/image-20250614124849333.png)

如上图所示，虽然样本点维度为 3 维，但决定样本点维度的特征可能主要由其中两个维度决定，将 3 维的样本映射到二维的子空间中，更容易学习到分类特征；

## 降维

降维的目的就是解决高维情况下出现的数据样本稀疏、距离计算困难等问题，但解决这些问题的同时，我们希望数据的结构不要有太大的变化，比如要求 **降维后样本之间的距离和原始空间中样本之间的距离要差不多**，就得到“多维缩放”（Multiple Dimensional Scaling，简称 MDS）降维算法。

### 多维缩放（Multiple Dimensional Scaling，简称 MDS）

假定 $m$ 个样本在原始空间的距离矩阵为 $D \in \mathbb{R}^{m \times m}$，其第 $i$ 行 $j$ 列的元素 $dist_{ij}$ 为样本 $x_i$ 到 $x_j$ 的距离。我们的目标是获得样本在 $d^\prime$ 维空间的表示 $Z \in \mathbb{R}^{d' \times m}$，$d' \leq d$，且任意两个样本在 $d^\prime$ 维空间中的欧氏距离等于原始空间中的距离，即 $\|z_i - z_j\| = dist_{ij}$。

令 $B = Z^TZ \in \mathbb{R}^{m \times m}$，其中 $B$ 为降维后样本的内积矩阵，$b_{ij} = z_i^T z_j$，有

$$
dist_{ij}^2 = \|z_i\|^2 + \|z_j\|^2 - 2z_i^T z_j \\
= b_{ii} + b_{jj} - 2b_{ij} \, .
\tag{10.3}
$$

为便于讨论，令降维后的样本 $Z$ 被中心化，即 $\sum\limits_{i=1}^m z_i = 0$。显然，矩阵 $B$ 的行与列之和均为零，即 $\sum\limits_{i=1}^m b_{ij} = \sum\limits_{j=1}^m b_{ij} = 0$。易知

$$
\sum_{i = 1}^m dist_{ij}^2 = \text{tr}(B) + mb_{jj} \, , 
\tag{10.4}
$$

$$
\sum_{j = 1}^m dist_{ij}^2 = \text{tr}(B) + mb_{ii} \, ,
\tag{10.5}
$$

$$
\sum_{i = 1}^m \sum_{j = 1}^m dist_{ij}^2 = 2m \, \text{tr}(B) \, .
\tag{10.6}
$$

其中 $\text{tr}(\cdot)$ 表示矩阵的迹（trace），$\text{tr}(B) = \sum\limits_{i=1}^m \|z_i\|^2$。**令**

$$
dist_{i \cdot }^2 = \frac{1}{m} \sum_{j = 1}^m dist_{ij}^2 \, ,
\tag{10.7}
$$

$$
dist_{\cdot j}^2 = \frac{1}{m} \sum_{i = 1}^m dist_{ij}^2 \, ,
\tag{10.8}
$$

$$
dist_{\cdot\cdot}^2 = \frac{1}{m^2} \sum_{i = 1}^m \sum_{j = 1}^m dist_{ij}^2 \,
\tag{10.9}
$$

<span style="background:#6fe7dd; border-radius:5px; display:inline-block;">由式(10.3)和式(10.4)~(10.9)可得</span>
$$
b_{ij} = -\frac{1}{2}(dist_{ij}^2 - dist_{i \cdot}^2 - dist_{\cdot j}^2 + dist_{\cdot\cdot}^2) \, ,
\tag{10.10}
$$

式（10.10）推导过程如下：

-   由式（10.6），$\frac{1}{m}tr(B)=\frac{1}{2m^2}\sum\limits_{i=1}^m\sum\limits_{j=1}^mdist_{ij}^2=\frac{1}{2}dist_{\cdot \cdot}^2$

-   由式（10.3），$b_{ij}=\frac{1}{2}(b_{ii}+b_{jj}-dist_{ij}^2)$

-   由式（10.4）、（10.5）

    -   $$
        b_{ii}=\frac{1}{m}\sum\limits_{j = 1}^mdist_{ij}^2-\frac{1}{m}tr(B)= dist_{i \cdot}^2-\frac{1}{2}dist_{\cdot \cdot}^2	\\
        b_{jj}=\frac{1}{m}\sum\limits_{i = 1}^mdist_{ij}^2-\frac{1}{m}tr(B)= dist_{ \cdot j}^2-\frac{1}{2}dist_{\cdot \cdot}^2	\\
        $$

-   所以：$b_{ij}=\frac{1}{2}(dist_{i \cdot}^2+dist_{\cdot j}^2-dist_{ij}^2-dist_{\cdot \cdot}^2)$

由此即可通过降维前后保持不变的距离矩阵 $D$ 求取内积矩阵 $B$。

对矩阵 $B$ 做特征值分解(eigenvalue decomposition)，$B = V \Lambda V^T$，其中 $\Lambda = \text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_d)$ 为特征值构成的对角矩阵，$\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_d$，$V$ 为特征向量矩阵。假定其中有 $d^*$ 个非零特征值，它们构成对角矩阵 $\Lambda_* = \text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_{d^*})$，令 $V_*$ 表示相应的特征向量矩阵，则 $Z$ 可表达为

$$
Z = \Lambda_*^{1/2} V_*^T \in \mathbb{R}^{d^* \times m} \, .
\tag{10.11}
$$

在现实应用中为了有效降维，往往仅需降维后的距离与原始空间中的距离尽可能接近，而不必严格相等。此时可取 $d' \ll d$ 个最大特征值构成对角矩阵 $\tilde{\Lambda} = \text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_{d'})$，令 $\tilde{V}$ 表示相应的特征向量矩阵，则 $Z$ 可表达为

$$
Z = \tilde{\Lambda}^{1/2} \tilde{V}^T \in \mathbb{R}^{d' \times m} \, .
\tag{10.12}
$$

下图给出了 MDS 算法的描述。![MDS 算法描述](https://cdn.jsdelivr.net/gh/Gongzihang6/Pictures@main/Medias/image-20250616193015341.png)



### 主成分分析（Principal Compoent Analysis，PCA）

PCA 是最常用的一种降维方法，在模式识别、金融、生物信息学等各个领域均得到广泛应用。

-   在机器学习本身的众多场景中也通常被用作数据预处理的首要方法；
-   发展出了各种变种（例如 Sparse PCA 、 Online PCA 、 Robust PCA 、Probabilistic PCA 等）和扩展工作；
-   当数据呈线性时， PCA 得到的结果是最优的；

#### 主要思想

-   通过线性组合方式快速提取变量的信息。当第一个线性组合不能提取更多的信息时，再用第二个线性组合继续提取，...，直到所提取的信息与原指标的信息差不多为止；主成分分析中的信息指的是变量（某一维度坐标）的变异性，用标准差或者方差衡量；

-   降维后的点需要能够对原始的样本点进行恰当的表达，因此 PCA 作为一种线性降维的方法，应该具有这样的性质：
    -   最近重构性：样本点到这个超平面的距离足够近；
    -   最大可分性：样本点在这个超平面上的投影尽可能分开；

#### 算法推导

PCA 降维的第一步是对样本进行标准化，即 $\sum_i x_i=0$，$var(x_i)=1$，再假定这一组样本经过投影变换后得到的新坐标系为 $\{w_1, w_2, \dots, w_d\}$，其中 $w_i$ 是<span style="color:#d59bf6;">标准正交基向量</span>，即 $||w_i||_2=1$，$w_i^Tw_j=0 \quad(i\neq j)$，若丢弃新坐标系中的部分坐标，将维度降低到 $d^\prime<d$，则样本点 $x_i$ 在低维坐标系中的投影是 $z_i=(z_{i1}; z_{i2}; \dots; z_{id^\prime})$，其中 $z_{ij}=w_j^Tx_i$ （即 $x_i$ 在 $w_j$ 方向上的投影）是 $x_i$ 在新坐标系下第 $j$ 维的坐标，若基于 $z_i$ 来重构 $x_i$，则会得到 $\hat{x_i}=\sum\limits_{j=1}^{d^{\prime}}z_{ij}w_j$

>   注意我们的优化目标是要找到一个新的坐标系 $\{w_1, w_2, \dots, w_{d^\prime}\}$，也就是降维得到的低维空间的一个坐标系，即降维后的点 $z_i$ 所在的空间的坐标系

考虑整个训练集，原样本点 $x_i$ 与基于投影重构的样本点 $\hat{x_i}$ 之间的距离为，我们的优化目标就是要最小化这个距离（等价于最小化降维带来的信息损失，因为如果不降维即 $d^\prime=d$，$z_i$ 可以完全重构 $x_i$，让 $\hat{x_i}=x_i$）
$$
\sum_{i = 1}^{m} \left\| \sum_{j = 1}^{d'} z_{ij} w_j - x_i \right\|_2^2 = \sum_{i = 1}^{m} {z}_i^\mathrm{T} {z}_i - 2 \sum_{i = 1}^{m} {z}_i^\mathrm{T} {W}^\mathrm{T} {x}_i + \text{const}



\propto -\text{tr} \left( {W}^\mathrm{T} \left( \sum_{i = 1}^{m} {x}_i {x}_i^\mathrm{T} \right) {W} \right).
$$
推导过程：

-   重构点 $\hat{x_i}=\sum\limits_{j=1}^{d^\prime}z_{ij}w_j=Wz_i$，其中 $W=[w_1, w_2, \dots, w_{d^\prime}] \in \mathbb{R}^{d\times d^\prime}$
-   利用 $z_{ij}=w_j^Tx_i$，可得：$\hat{x_i}=\sum\limits_{j=1}^{d^\prime}w_jw_j^Tx_i=WW^Tx_i$，	$z_i=W^Tx_i$

-   $W^TW=I$，因为 $w_j$ 是正交单位向量；所以 $WW^TWW^T=WW^T$;

所以 $\sum\limits_{i=1}^{m} \left\| \sum\limits_{j=1}^{d'} z_{ij} w_j - x_i \right\|_2^2 = \sum\limits_{i=1}^m\left\|Wz_i-x_i\right\|_2^2$，即可得到式（5）的结果

根据矩阵迹的性质，$-2\sum\limits_{i=1}^mz_i^TW^Tx_i=-2\sum\limits_{i=1}^mx_i^TWW^Tx_i=-2tr(\sum\limits_{i=1}^mx_i^TWW^Tx_i)=-2tr(W^T(\sum\limits_{i=1}^mx_ix_i^T)W)$



所以优化目标可以写成：
$$
\mathop{\max}\limits_{W} \quad tr(W^TXX^TW)	\\
s.t. \quad W^TW = I
$$
其中 $W=[w_1, w_2, \dots, w_{d^\prime}] \in \mathbb{R}^{d\times d^\prime}$，$X=[x_1, x_2, \dots, x_m]\in \mathbb{R}^{d\times m}$

用拉格朗日乘子法求解上述优化问题，定义拉格朗日函数 $\mathcal{L}$ 如下：
$$
\mathcal{L}(W,\Lambda)= tr(W^TXX^TW)-tr(\Lambda^T(W^TW-I))
$$
其中 $\Lambda$ 是拉格朗日乘子矩阵，与约束 $W^TW=I$ 相关联

接下来对 $W$ 和 $\Lambda$ 求导，令导数为 0 得到极值点，
$$
\frac{\partial \mathcal{L}}{\partial W}= 2XX^TW-2\Lambda W = 0	\\
==> \quad XX^TW =\Lambda W
$$

$$
\frac{\partial \mathcal{L}}{\partial \Lambda}= W^TW-I = 0	\\
==> \quad W^TW = I
$$

对于式（8），由于 $XX^T$ 是对称矩阵，同时 W 又是正交矩阵，所以可以将式（8）看成矩阵特征值问题，我们只需要对协方差矩阵 XX^T 进行特征值分解，将求得的特征值排序：$\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_d$，再取前 $d^\prime$ 个特征值对应的特征向量构成 $W=(w_1, w_2, \dots, w_{d^\prime})$，就是该优化问题的解

>   实践中通常对 $XX^T$ 进行奇异值分解来代替协方差矩阵的特征值分解

![PCA 算法步骤](https://cdn.jsdelivr.net/gh/Gongzihang6/Pictures@main/Medias/image-20250616110753732.png)

降维后低维空间的维数 $d^\prime$ 通常是由用户事先指定，或通过在值 $d^\prime$ 不同的低维空间中对 k 近邻分类器(或其他开销较小的学习器) 进行交叉验证来选取较好的 $d^\prime$ 值. 对 PCA，还可从重构的角度设置一个重构阈值，例如 ( t = 95% \)，然后选取使下式成立的最小 $d^\prime$ 值：

$$
\frac{\sum_{i = 1}^{d'} \lambda_i}{\sum_{i = 1}^{d} \lambda_i} \geq t \tag{10.18}
$$

### 核化 PCA



### 流形降维

“流形”是在局部与欧式空间同胚的空间。流形的本质是局部平坦，全局弯曲，整体上可能有曲率、洞或复杂结构，欧式距离无法反映真实“最短路径”，但 **局部每一点附件都可以近似为欧式空间，可以用欧式距离来度量最短路径**；

给降维带来的启发：若低维流形嵌入到高维空间中，则数据样本在高维空间的分布虽然看上去非常复杂，但在局部仍具有欧式空间的性质，因此可以容易的在局部建立降维映射关系，然后再设法将局部映射关系推广到全局。

这里介绍两种著名的流形学习方法，等度量映射和局部线性嵌入。

#### 等度量映射（Isometric Mapping，简称 Isomap）

基本思想：希望在映射过程中保持测地线的距离。（这里面分析的降维方法都是为了更好的进行 k 近邻分类，方便距离的计算，因此为了满足 k 近邻分类的基本假设“近朱者赤，近墨者黑”，我们都希望降维前后，原来距离近的样本降维后还是近，原来距离远的样本降维后还是远）；

Isomap 在流形结构未知的情况下，采用构造邻接图（Graph）来通过有限的数据来估算流形上的测地线；

<img src="https://cdn.jsdelivr.net/gh/Gongzihang6/Pictures@main/Medias/image-20250616195144452.png" alt="使用图结构来估算测地线" style="zoom:50%;" />

如图所示，在数据有限的情况下，我们无法获取流形的真实结构，即表面的真实曲率、是否有空洞等等，所以我们无法获得流形上真实的测地线距离；所以 Isomap 通过将有限的数据样本点连接为图结构，具体操作为：对每一个样本点 $x_i$，确定 $x_i$ 的 k 近邻，将 $x_i$ 与 k 近邻点之间的距离设置为欧式距离，与其他点的距离设置为无穷大；然后连接 $x_i$ 与它的 k 近邻点，这样就形成了如图所示的图结构。

之后任意两个点之间的测地距离就可以通过图中这两个点之间的最短路径距离来近似了。

Isomap 算法描述如下：

![Isomap 算法描述](https://cdn.jsdelivr.net/gh/Gongzihang6/Pictures@main/Medias/image-20250616195816976.png)

对近邻图的构建通常有两种做法，一种是指定近邻点个数，例如欧氏距离最近的 $k$ 个点为近邻点，这样得到的近邻图称为 $k$ 近邻图；另一种是指定距离阈值 $\epsilon$，距离小于 $\epsilon$ 的点被认为是近邻点，这样得到的近邻图称为 $\epsilon$ 近邻图。两种方式均有不足，例如若 **近邻范围指定得较大，则距离很远的点可能被误认为近邻**，这样就出现 **“短路”** 问题；**近邻范围指定得较小，则图中有些区域可能与其他区域不存在连接**，这样就出现 **“断路”** 问题。短路与断路都会给后续的最短路径计算造成误导。

>**邻接图构建：为何先用欧式距离？**
>
>-   **输入**：高维数据集 $D={x_1,x_2,...,x_m}$，近邻参数 k。
>-   操作：
>    1.  **计算欧式距离**：对每个点 $x_i$，计算它与所有其他点的欧式距离 $d_E(x_i,x_j)$。
>    2.  **选择 k 近邻**：保留距离最近的 k 个点作为邻居，其他点的距离设为无穷大（即不直接连接）。
>-   **输出**：得到一个稀疏的邻接图，边权为欧式距离（仅限 k 近邻之间）。
>
>**关键点**：
>
>-   **局部合理性**：在足够小的邻域内，流形近似平坦，欧式距离误差较小。
>-   **计算效率**：欧式距离计算简单，适合快速构建初始图结构。

#### 局部线性嵌入（Locally Linear Embedding，简称 LLE）

基本思想：与 **Isomap 试图保持近邻样本之间的距离** 不同，局部线性嵌入（Locally Linear Embedding，简称 LLE）**试图保持邻域内样本之间的线性关系。**

“流形在局部可以近似等价于欧氏空间”便是 LLE 分析方法的出发点。LLE 和 Isomap 类似，都基于局部同胚于欧氏空间的假设

-   **局部线性假设**：流形在局部邻域内近似为欧式空间，样本点与其近邻点可通过线性组合表示（如平面上的点可用相邻点加权求和表示）
-   **全局非线性降维**：通过保留这些局部线性关系，将数据映射到低维空间，同时避免直接计算全局距离（如 Isomap 的测地线距离）

算法流程如下：

![高维空间中的样本重构关系在低维空间中得以保持](https://cdn.jsdelivr.net/gh/Gongzihang6/Pictures@main/Medias/image-20250616201911786.png)

假定样本点 $x_i$ 的坐标能够通过它的邻域样本 $x_j，x_k，x_l$ 的坐标通过线性组合而重构出来，即
$$
x_i = w_{ij}x_j+w_{ik}x_k+w_{il}x_l
$$
LLE 希望式(9)的关系在低维空间中得以保持。

LLE 先为每个样本 $x_i$ 找到其近邻下标集合 $Q_i$，然后计算出基于 $Q_i$ 中的样本点对 $x_i$ 进行线性重构的系数 $w_i$：

$$
\min_{w_1, w_2, \ldots, w_m} \sum_{i = 1}^m \left\| x_i - \sum_{j \in Q_i} w_{ij} x_j \right\|_2^2 \tag{10.27}
$$

s.t. $\sum_{j \in Q_i} w_{ij} = 1$,

其中 $x_i$ 和 $x_j$ 均为已知，令 $C_{jk} = (x_i - x_j)^T(x_i - x_k)$，$w_{ij}$ 有闭式解

$$
w_{ij} = \frac{\sum_{k \in Q_i} C_{jk}^{-1}}{\sum_{l, s \in Q_i} C_{ls}^{-1}} \, . \tag{10.28}
$$

LLE 在低维空间中保持 $w_i$ 不变，于是 $x_i$ 对应的低维空间坐标 $z_i$ 可通过下式求解：

$$
\min_{z_1, z_2, \ldots, z_m} \sum_{i = 1}^m \left\| z_i - \sum_{j \in Q_i} w_{ij} z_j \right\|_2^2 \, . \tag{10.29}
$$

式(10.27)与(10.29)的优化目标同形，唯一的区别是式(10.27)中需确定的是 $w_i$，而式(10.29) 中需确定的是 $x_i$ 对应的低维空间坐标 $z_i$。

令 $Z = (z_1, z_2, \ldots, z_m) \in \mathbb{R}^{d' \times m}$，$(W)_{ij} = w_{ij}$，

$$
M = (I - W)^T(I - W) \, , \tag{10.30}
$$

则式(10.29)可重写为

$$
\min_Z \text{tr}(ZMZ^T) \, , \tag{10.31}
$$

s.t. $ZZ^T = I$。

式(10.31)可通过特征值分解求解：M 最小的 $d'$ 个特征值对应的特征向量组成的矩阵即为 $Z^T$。

LLE 的算法描述如下图所示。算法第 4 行显示出：对于不在样本 $x_i$ 邻域区域的样本 $x_j$，无论其如何变化都对 $x_i$ 和 $z_i$ 没有任何影响；这种将变动限制在局部的思想在许多地方都有用

![LLE算法步骤描述](https://cdn.jsdelivr.net/gh/Gongzihang6/Pictures@main/Medias/image-20250616205231610.png)

Isomap vs LLE

相同点：

-   Isomap 和 LLE 从不同的出发点来实现同一个目标，它们都能从某种程度上发现并在映射的过程中保持流形的几何性质。

不同点：

-   Isomap 希望保持任意两点之间的测地线距离； LLE 希望保持局部线性关系。

-   从保持几何的角度来看， Isomap 保持了更多的信息量


然而 Isomap 的 全局 方法有一个很大的问题就是要考虑任意两点之间的关系，这个数量将随着数据点数量的增多而爆炸性增长，从而使得计算难以负荷。因此，以 LLE 为开端的局部分析方法的变种和相关的理论基础研究逐渐受到更多的关注。



#### LE（Laplacian Eigenmap）

Laplacian Eigenmap (LE ) 希望保持流形的近邻关系 : 将原始空间中相近的点映射成目标空间中相近的点。







![image-20250625120829043](https://cdn.jsdelivr.net/gh/Gongzihang6/Pictures@main/Medias/image-20250625120829043.png)







