[TOC]

# 模型评估与多分类

## 泛化性能

-   现实任务中往往会对学习器的泛化性能、时间开销、存储开销、可解释性等方面的因素进行评估并作出选择；

通常我们把分类错误的样本数占样本总数的比例称为“错误率”（error rate），即如果在 $m$ 个样本中有 $a$ 个样本分类错误，则错误率为 $E=a/m$；相应的，$1-a/m$ 称为“精度”（accuracy），即精度 = 1-错误率；

泛化（推广，generalization）性能是指学习机器对“新样本”预测的能力，也是机器学习的目标；学习器实际预测输出与样本的真实输出之间的差异称为“误差”，我们把学习器在训练集上的误差称为“训练误差”（training error）或者“经验误差”（emprical error）；在新样本下的误差称为“泛化误差”（generalization error），我们希望得到泛化误差小的学习器（因为我们实际应用学习器的时候，测试的样本肯定是学习器没有见过的，我们希望学习器在没有见过的新样本上依旧有很好的性能）

机器学习通常假设样本空间服从一个存在但未知的分布，样本从这个分布中独立获得，即独立同分布；一般而言，训练样本越多，越有可能通过学习获得强泛化能力的模型；

由于事先并不知道“新样本”的信息，我们只能努力使经验误差最小化，并将测试集上的“测试误差”作为泛化误差的近似，所以测试集要和训练集中的样本尽可能互斥；



### 误差分解

“偏差-方差分解”（bias-variance decomposition）是解释机器学习算法泛化性能的一种重要工具；

对测试样本 $x$，令 $y_D$ 为 $x$ 在数据集中的标记，$y$ 为 $x$ 的真实标记，$f(x;D)$ 为训练集 D 上学到的模型 $f$ 在 $x$ 上的预测输出，以回归任务为例，学习算法的期望预测为<span style="color:#d59bf6;">（这里指的是同一个学习算法在**样本数相同的训练集**上训练出来的模型的产生的预测的期望值）</span>
$$
\bar{f}(x)= E_D [f(x; D)]
$$
使用样本数相同的不同训练集产生的方差为
$$
var(x)= E_D [(f(x; D)-\bar{f}(x))^2]
$$
噪声，即 x 在数据集中的标记与真实标记的差，可以理解为观测误差或者测量误差
$$
\epsilon^2 = E_D [(y_D-y)^2]
$$


期望输出与真实标记的差别称为偏差（bias），即
$$
\text{bias}^2{(x)}=(\bar{f}(x)-f(x))^2
$$
为了便于讨论，假定噪声期望为 0，即 $E_D[y_D-y]=0$，通过简单的多项式展开合并，可以对算法的期望泛化误差进行分解：



$$
\begin{aligned}
E(f; D) &= \mathbb{E}_D \left [ (f(x; D) - y_D)^2 \right] \\
&= \mathbb{E}_D \left [ (f(x; D) - \bar{f}(x) + \bar{f}(x) - y_D)^2 \right] \\
&= \mathbb{E}_D \left [ (f(x; D) - \bar{f}(x))^2 \right] + \mathbb{E}_D \left [ (\bar{f}(x) - y_D)^2 \right]  + \mathbb{E}_D \left [ 2(f(x; D) - \bar{f}(x))(\bar{f}(x) - y_D) \right] \\
&= \mathbb{E}_D \left [ (f(x; D) - \bar{f}(x))^2 \right] + \mathbb{E}_D \left [ (\bar{f}(x) - y_D)^2 \right] \\
&= \mathbb{E}_D \left [ (f(x; D) - \bar{f}(x))^2 \right] + \mathbb{E}_D \left [ (\bar{f}(x) - y + y - y_D)^2 \right] \\
&= \mathbb{E}_D \left [ (f(x; D) - \bar{f}(x))^2 \right] + \mathbb{E}_D \left [ (\bar{f}(x) - y)^2 \right] + \mathbb{E}_D \left [ (y - y_D)^2 \right] 
 + 2\mathbb{E}_D \left [ \bar{f}(x) - y)(y - y_D) \right] \\
&= \mathbb{E}_D \left [ (f(x; D) - \bar{f}(x))^2 \right] + (\bar{f}(x) - y)^2 + \mathbb{E}_D \left [ (y_D - y)^2 \right] \, 
\end{aligned}
$$


所以，
$$
E(f; D)= var^2(x)+bias^2(x)+\epsilon^2
$$
即泛化误差可以分解为：方差、偏差、噪声之和；

其中，偏差度量了学习算法的期望预测与数据集标记的偏离程度，刻画了学习算法本身的拟合能力（因为学习算法的目标是向着数据集的标记靠近）；方差度量了同样大小的训练集的变动导致的学习性能的变化，刻画了数据扰动对算法学习性能的影响；噪声则表达了在当前任务下任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度；

偏差-方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度共同决定的。

给定学习任务，为了取得好的泛化性能，则需要使偏差比较小，即能够充分拟合数据，并且使方差小，即使得数据扰动产生的影响小；

![偏差-方差窘境](https://cdn.jsdelivr.net/gh/Gongzihang6/Pictures@main/Medias/image-20250619211138029.png)

图一说明，给定学习任务，假定我们能控制学习算法的训练程度，则在训练不足时，学习器的拟合能力不够强，训练数据的扰动不足以使学习器产生显著变化（也就是同样大小，给我不同的训练集，我的拟合能力比较差，学出来没有什么太大区别，主要是学的太差，和真实答案相距太远），此时偏差主导了泛化错误率；随着训练程度的加深，学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学到，方差逐渐主导了泛化错误率；在训练程度充足后，学习器的拟合能力已非常强，训练数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全局的特性被学习器学到了，则将发生过拟合。



### 过拟合与欠拟合

过拟合：指的是学习器把训练样本学的“太好了”的时候，很可能已经把训练样本自身的一些独有的特点当作了所有潜在样本都会具有的一般性质，这样就会导致泛化性能的下降；

-   优化学习目标，加正则项，抑制模型的复杂程度；
-   早停机制，early stop，当出现模型在验证集上误差不降反升时，及时停止训练；

欠拟合：指的是学习器对训练样本的一般性质尚未学号；

-   决策树：拓展分支，增强模型学习能力；
-   神经网络：增加训练轮数，加深模型学习程度

![过拟合、欠拟合的直观类比](https://cdn.jsdelivr.net/gh/Gongzihang6/Pictures@main/Medias/image-20250619211807194.png)



## 评估方法

### 留出法

将数据集 D 划分为两个互斥的集合，其中一个集合作为训练集 S，另一个作为测试集 T，即 $D=S \cup T$，$S\cap T=\varnothing$，在 S 上训练出模型后，用 T 来评估其测试误差，作为对泛化误差的估计

>   训练/测试集的划分要尽可能保持数据分布的一致性，避免因为数据划分过程引入额外的偏差而对最终结果产生影响，解决方式为 **分层采样**

注意：

1、即便在给定训练集/测试集的样本比例下，仍存在多种划分方式对初始训练集 D 进行分割，这些不同的划分将导致不同的训练集/测试集，学习到的模型也就不一样，相应的，模型评估的结果也会有差别，因此单次的留出法得到的结果往往不够稳定可靠，在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果；

2、我们希望评估的是用 D 训练出的模型的性能，但留出法需要划分训练/测试集，这就会导致一个窘境：

-   若让训练集 S 包含绝大多数样本，则训练出的模型更接近与用 D 训练出的模型，但由于测试集 T 比较小，评估结果可能不够稳定准确；（从偏差-方差分解角度来看，测试集小时，评估结果的偏差较大）
-   若令测试集 T 多包含一些样本，则训练集 S 与 D 差别更大了，被评估的模型与用 D 训练出的模型相比可能有较大差别；（一般而言，测试集至少应包含 30 个样例）



### 交叉验证法

交叉验证法（cross validation）先将数据集 D 划分为 k 个大小相似的互斥子集，即 $D=D_1 \cup D_2 \cup \dots \cup D_k$，$D_i \cap D_j=\varnothing$ $(i \neq j)$，每个子集 $D_i$ 都尽可能保持数据分布的一致性，即从 D 中分层采样得到，然后用其中 k-1 个子集作为训练集，剩下一个子集作为测试集，这样就可以获得 k 组训练集/测试集，从而可以进行 k 次训练和测试，最终返回的是这 k 个测试结果的平均值；

![10 折交叉验证示意图](https://cdn.jsdelivr.net/gh/Gongzihang6/Pictures@main/Medias/image-20250619213314549.png)

假定数据集 $D$ 中包含 $m$ 个样本，若令 $k=m$，则得到了交叉验证法的一个特例：留一法（Leave-One-Out，简称 LOO）

留一法的优点：

-   实际评估的模型与期望评估的用 $D$ 训练出来的模型很相似，因此留一法的评估结果往往被认为比较准确；

留一法的缺陷：

-   数据集比较大时，训练 $m$ 个模型的计算开销可能是无法忍受的（例如训练集包含一百万个样本，则需要训练一百万个模型）；

### 自助法

以自助采样（bootstrap sampling）为基础，给定包含 $m$ 个样本的数据集 $D$，我们对他进行采样产生数据集 $D^\prime$：每次随机从 $D$ 中挑选一个样本，将其拷贝放入 $D^\prime$，然后再将该样本放回初始数据集 $D$ 中，使得该样本下次采样时仍有可能被采到；这个过程重复 $m$ 次，我们就得到了包含 $m$ 个样本的数据集 $D^\prime$

这就是自助采样的过程，显然 $D$ 中有一部分样本会在 $D^\prime$ 中多次出现，而另一部分样本不出现；从概率视角出发，一个样本在 m 次采样中始终不被采到的概率是 $(1-\frac{1}{m})^m$，取极限得到
$$
\lim_{m \to \infty}(1-\frac{1}{m})^m \to \frac{1}{e} \approx 0.368
$$
也就是通过自助采样，初始数据集 D 中约有 36.8%的样本未出现在采样数据集 $D^\prime$ 中，于是可以将 $D^\prime$ 作为训练集，$D/D^\prime$ 作为测试集；

应用场景：

-   自助法在数据集较小、难以有效划分训练集/测试集时很有用；
-   自助采样可以从初始数据集中产生多个不同的训练集，对集成学习有很大的好处；

缺点：自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差；

## 性能度量

也就是模型的评价标准

在预测任务中，给定样例集 $D={(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)}$，其中 $y_i$ 是示例 $x_i$ 的真实标记，通过比较学习器的预测结果 f(x)与真实标记 y 来评估模型的性能

对于回归任务，常用均方误差
$$
E(f; D)=\frac{1}{m}\sum_{i = 1}^m(f(x_i)-y_i)^2
$$
更一般的，对于数据分布 $\mathcal{D}$ 和概率密度函数 $p(\cdot)$，均方误差可描述为

$$
E(f; \mathcal{D}) = \int_{x \sim \mathcal{D}} (f(x) - y)^2 p(x) \mathrm{d}x.
$$

### 错误率与精度

错误率
$$
E(f; D) = \frac{1}{m} \sum_{i=1}^{m} \mathbb{I}(f(x_i) \neq y_i) \ .
$$
精度
$$
\begin{aligned}
\text{acc}(f; D) &= \frac{1}{m} \sum_{i=1}^{m} \mathbb{I}(f(x_i) = y_i) \\
&= 1 - E(f; D) \, .
\end{aligned}
$$
更一般的，对于数据分布 $\mathcal{D}$ 和概率密度函数 $p(\cdot)$，错误率与精度可分别描述为

$$
E(f; \mathcal{D}) = \int_{x \sim \mathcal{D}} \mathbb{I}(f(x) \neq y) p(x) \mathrm{d}x 
$$

$$
\begin{aligned}
\text{acc}(f; \mathcal{D}) &= \int_{x \sim \mathcal{D}} \mathbb{I}(f(x) = y) p(x) \mathrm{d}x \\
&= 1 - E(f; \mathcal{D}) \, .
\end{aligned}
$$

### 查准率、查全率与 F1

查准率亦称“准确率”，指的是查出来的<span style="color:#d59bf6;">有多少是我想要的</span>，**我想要的**<span style="background:#6fe7dd; border-radius:5px; display:inline-block;">占</span>**你查出来的**多少百分比；(预测为正的样例中真正是正例的样本占多少)

查全率亦称“召回率”，指的是<span style="color:#d59bf6;">我想要的有多少被查出来了</span>，也就是**你查出来的我想要的**<span style="background:#6fe7dd; border-radius:5px; display:inline-block;">占</span>**全部我想要的**的百分比；（真正的正例样本有多少被预测出来了）

根据实际场景的不同，对这两个指标的重视程度也不一样；

对于二分类问题，可以将样例根据其真实类别与学习器预测的类别，分为真正例（true positive，真阳性，标记为真的同时学习器预测也为真，TP），假正例（false positive，假阳性，标记为假的同时但是学习器预测为真，FP），真反例（true negative，真阴性，标记为假的同时学习器预测也为假），假反例（false negative，假阴性，标记为真的同时但是学习器预测为假），显然有`TP+FP+TN+FN=样例总数`，可以用混淆矩阵来描述这个结果

![二分类结果混淆矩阵](https://cdn.jsdelivr.net/gh/Gongzihang6/Pictures@main/Medias/image-20250621094040885.png)

根据前面的定义，查准率P和查全率R分别定义为：
$$
P=\frac{TP}{TP+FP}	\\ \\
R=\frac{TP}{TP+FN}
$$
查准率和查全率是一对矛盾的变量；一般来说，查准率高时，查全率往往偏低，而查全率高时，查准率往往偏低；例如，如果希望查准率高，也就是挑选出来的西瓜中好瓜比例尽可能高，则学习器就会只挑选最有把握的瓜，但这样就难免会漏掉不少好瓜，使得查全率比较低；如果希望查全率高，也就是所有的好瓜都尽可能地被选出来，则学习器可以通过增加选瓜地数量地来实现，极端情况就是将所有地瓜都选上，那所有的好瓜自然都被选上了，查全率为100%，但是查准率就比较低了；

在很多情形下，我们可根据学习器的预测结果对样例进行排序，排在前面的是学习器认为“最可能”是正例的样本，排在最后的则是学习器认为“最不可能”是正例的样本，按此顺序逐个把样本作为正例进行预测，则每次可以计算出当前的查全率、查准率。

以查全率为横轴、查准率为纵轴作图，就得到PR曲线

<img src="https://cdn.jsdelivr.net/gh/Gongzihang6/Pictures@main/Medias/image-20250621102915213.png" alt="PR曲线与平衡点示意图" style="zoom:80%;" />

PR图直观的显示出学习器在样本总体上的查全率、查准率；在进行比较时，如果一个学习器的PR曲线被另一个学习器完全“包住”，则可断言后者的性能优于前者，如图中A的性能优于C，可以这么理解，在绘图过程中，我们是按照学习器的预测结果对样例进行排序的，从第一个样本开始，每预测一个样本，如果这个样本预测正确了，则查准率为100%，查全率也从0开始增加到非0，并逐渐增加，如果这个样本预测错误了，则查准率开始下降，而查全率不变，显示在PR图上就是曲线开始下降，因此C下降速度明显比A快，说明C有更多样本预测错误；

A和B学习器的性能度量就要通过平衡点（Break-Event Point，简称BEP），它是查准率=查全率时的取值，例如图中的学习器C的BEP是0.64，基于BEP的比较，可认为学习器A优于B，但是BEP还是过于简化了，更常用的是F1度量
$$
F1=\frac{2\times P\times R}{P+R}=\frac{2\times \frac{TP}{TP+FP}\times \frac{TP}{TP+FN}}{\frac{TP}{TP+FP}+\frac{TP}{TP+FN}}=\frac{2\times TP}{样例总数+TP-TN}
$$
在一些应用中，对查准率和查全率的重视程度有所不同，例如在商品推荐系统中，为了尽可能少打扰用户，更希望推荐内容确实是用户感兴趣的，此时查准率更重要（也就是希望推荐内容确实是用户感兴趣的，而不是将用户感兴趣的内容都推荐出来）；而在逃犯信息检索系统中，更希望尽可能少漏掉逃犯，此时查全率更重要；

F1度量的一般形式$F_{\beta}$，能让我们表达出对查准率/查全率的不同偏好，它定义为
$$
F_{\beta}=\frac{(1+\beta^2)\times P \times R}{(\beta^2\times P)+R}
$$
其中$\beta>0$度量了查全率对查准率的相对重要性，$\beta=1$退化为标准的F1，$\beta>1$是查全率有更大影响；$\beta<1$时查准率有更大影响

一种直接的做法是先在各混淆矩阵上分别计算出查准率和查全率，记为 $(P_1, R_1), (P_2, R_2), \ldots, (P_n, R_n)$，再计算平均值，这样就得到“宏查准率” (macro-$P$)、“宏查全率” (macro-$R$)，以及相应的“宏$F1$” (macro-$F1$):

$$
\text{macro-}P = \frac{1}{n} \sum_{i=1}^{n} P_i 
$$

$$
\text{macro-}R = \frac{1}{n} \sum_{i=1}^{n} R_i 
$$

$$
\text{macro-}F1 = \frac{2 \times \text{macro-}P \times \text{macro-}R}{\text{macro-}P + \text{macro-}R}
$$

还可将各混淆矩阵的对应元素进行平均，得到 $TP$、$FP$、$TN$、$FN$ 的平均值，分别记为 $\overline{TP}$、$\overline{FP}$、$\overline{TN}$、$\overline{FN}$，再基于这些平均值计算出“微查准率” (micro-$P$)、“微查全率” (micro-$R$) 和“微$F1$” (micro-$F1$):

$$
\text{micro-}P = \frac{\overline{TP}}{\overline{TP} + \overline{FP}} 
$$

$$
\text{micro-}R = \frac{\overline{TP}}{\overline{TP} + \overline{FN}} 
$$

$$
\text{micro-}F1 = \frac{2 \times \text{micro-}P \times \text{micro-}R}{\text{micro-}P + \text{micro-}R}
$$

### ROC 与 AUC

很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类与之（threshold）进行比较，若大于阈值则分为正类，否则为反类；实际上，根据这个实值或者概率预测结果，我们可以将测试样本进行排序，“最可能”是正例的排在最前面，“最不可能”是正例的排在最后面，这样分类过程就相当于在这个排序中以某个截断点将样本分为两个部分，前一部分为正例，后一部分为反例；

在不同任务中，我们可根据任务需求来采用不同的截断点，如果我们更重视查准率，则我们应该选择排序中靠前的位置来进行截断，如果我们更重视查全率，则我们应该选择排序中靠后的位置进行截断；

ROC全称是“受试者工作特征”（Receiver Operating Characteristic）曲线，与PR曲线相似，我们根据学习器的预测结果对样例进行排序，按此顺序逐个把样本作为正例进行预测，每次计算出两个重要量的值，分别以它们为横、纵坐标作图，就得到了**ROC曲线图**，其横轴是“假正例率”（False Positive Rate，简称FPR），纵轴是“真正例率”（True Positive Rate，简称TPR）；FPR和TPR定义如下：
$$
FPR=\frac{FP}{TN+FP}
$$
FP是假阳，也就是将负例样本预测为了正例样本，TN+FP则是总的负样本的数量，因此FPR表示总的负样本中预测错误的比例；
$$
TPR=\frac{TP}{TP+FN}
$$
TP表示真阳性，也就是将正例样本预测为正例样本，TP+FN则是总的正例样本的数量，因此TPR表示总的正例样本中预测正确的比例；

绘制出如下图所示的近似ROC曲线。绘图过程很简单：给定 $m^+$ 个正例和 $m^-$ 个反例，**根据学习器预测结果对样例进行排序**，然后把分类阈值设为最大，即把所有样例均预测为反例，此时真正例率和假正例率均为 0，在坐标 $(0,0)$ 处标记一个点。然后，将分类阈值依次设为每个样例的预测值，即**依次将每个样例划分为正例**。设前一个标记点坐标为 $(x,y)$，当前若为真正例，则对应标记点的坐标为 $(x, y + \frac{1}{m^+})$；当前若为假正例，则对应标记点的坐标为 $(x + \frac{1}{m^-}, y)$，然后用线段连接相邻点即得。

![ROC曲线与AUC示意图](https://cdn.jsdelivr.net/gh/Gongzihang6/Pictures@main/Medias/image-20250622122355434.png)

如何根据ROC曲线比较学习器性能的优劣？与PR曲线类似，若一个学习器的ROC曲线被另一个学习器的曲线完全包住，则可以断言后者的性能优于前者；若两个学习器的ROC曲线发生交叉，则难以一般性的断言两者孰优孰劣，可以通过比较ROC曲线下的面积，即AUC（Area Under ROC Curve）

从定义可知，AUC可以通过求ROC曲线下各部分的面积之和得到，相当于求多个梯形的面积之和。
$$
AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)(y_i+y_{i+1})
$$
形式化的看，AUC考虑的是样本预测的排序质量，因此它与排序误差有紧密联系。给定m^+个整理和m^-个反例，令D^+和D^-分别表示正、反例集合，则排序损失定义为：
$$
l_{rank}=\frac{1}{m^+ m^-}\sum_{x^+ \in D^+}\sum_{x^- \in D^-}\left(\mathbb{I}\left(f(x^+)<f(x^-)\right)+\frac{1}{2}\mathbb{I}\left(f(x^+)=f(x^-)\right)\right)
$$
也就是考虑每一对正、反例，若正例的预测值小于反例，则计一个罚分，若相等，则计0.5个罚分，容易看出，l_{rank}对应的是ROC曲线上方的面积：若一个正例在ROC曲线上对应标记点的坐标为(x,y)，则x恰是排序在其之前的反例所占的比例，即假正例率。因此有
$$
AUC=1-\mathscr{l}_{rank}
$$


---

好的，这张图片是来自一本关于机器学习的经典教材（很可能是周志华老师的《机器学习》，俗称“西瓜书”），它讨论了ROC曲线和AUC的定义与计算。

我会结合图片中的内容，用一个通俗易懂的例子，为你详细解释ROC曲线和AUC的实际意义。

#### 核心思想：从一个医生的诊断说起

想象一下，你开发了一个AI模型，用来帮助医生判断CT影像中是否有肿瘤。这个模型不像人类医生直接说“有”或“没有”，而是输出一个**“可能性”得分**，比如0到1之间的一个数值。得分越高，模型认为有肿瘤的可能性越大。

现在，医生需要根据这个得分来做最终诊断。他可以设定一个**“阈值（Threshold）”**。

*   **如果阈值设为0.8：** 只有得分 > 0.8的影像，AI才判定为“有肿瘤”。
*   **如果阈值设为0.3：** 只要得分 > 0.3的影像，AI就判定为“有肿瘤”。

你会发现，阈值的选择至关重要，它直接导致了两种我们不希望发生的情况：

1.  **漏诊 (False Negative, FN):** 病人明明有肿瘤，但模型得分低于阈值，被判定为“没有”。这很危险！
2.  **误诊 (False Positive, FP):** 病人很健康，但模型得分高于阈值，被判定为“有肿瘤”。这会给病人带来不必要的恐慌和进一步检查。

**ROC曲线和AUC就是用来评估模型在这种“权衡取舍”下的整体表现的工具。**

---

##### 第一部分：理解ROC曲线 (Receiver Operating Characteristic Curve)

要理解ROC曲线，我们先要定义它的两个坐标轴：

1.  **纵坐标 (Y轴)：真正例率 (True Positive Rate, TPR)**
    *   **公式：** TPR = (真正被模型识别为正例的数量) / (所有真正为正例的数量)
    *   **在医生例子中：** **“所有真病人中，被AI正确揪出来的比例”**。也被称为“灵敏度(Sensitivity)”或“召回率(Recall)”。
    *   **我们希望它怎么样？** 当然是越高越好，最好是1 (100%)，代表没有漏诊。

2.  **横坐标 (X轴)：假正例率 (False Positive Rate, FPR)**
    *   **公式：** FPR = (被模型错误识别为正例的负例数量) / (所有真正为负例的数量)
    *   **在医生例子中：** **“所有健康人中，被AI误诊为病人的比例”**。
    *   **我们希望它怎么样？** 当然是越低越好，最好是0，代表没有误诊。

##### 如何绘制ROC曲线？

ROC曲线描绘的是**当阈值从高到低连续变化时，TPR和FPR组成的点所连成的曲线。**

1.  **阈值设为最高(比如1.0)：** 模型非常“苛刻”，几乎把所有样本都判为“无肿瘤”。这时TPR和FPR都接近0。（对应ROC图的左下角(0,0)点）
2.  **逐渐降低阈值：** 模型变得越来越“宽松”。
    *   一些得分较高的真病人开始被正确识别，TPR上升。
    *   同时，一些得分较高的健康人也可能被误诊，FPR也开始上升。
    .
3.  **阈值设为最低(比如0.0)：** 模型非常“宽松”，把所有样本都判为“有肿瘤”。这时所有真病人都被找出(TPR=1)，同时所有健康人也都被误诊(FPR=1)。（对应ROC图的右上角(1,1)点）

将阈值变化过程中所有(FPR, TPR)的点连接起来，就构成了这个模型的ROC曲线。

#### ROC曲线的实际意义

*   **一个点代表一种策略：** ROC曲线上的每一个点，都对应一个具体的阈值，代表了在该阈值下“漏诊”和“误诊”的一种平衡。
*   **曲线的形状代表模型的能力：**
    *   **理想模型：** 曲线会无限逼近左上角(0, 1)点。这意味着我们能找到一个阈值，使得TPR为1（没有漏诊），同时FPR为0（没有误诊）。这是完美的分类器。
    *   **随机猜测模型：** 曲线是一条从(0,0)到(1,1)的对角线（y=x）。这意味着，要想多揪出一个真病人，就必须接受多误诊一个健康人。这种模型没有任何诊断价值。
    *   **曲线越靠近左上角，说明模型性能越好。**



---

##### 第二部分：理解AUC (Area Under the Curve)

正如图片中文字所述，当两个模型的ROC曲线发生交叉时，我们很难说哪个模型更好。这时就需要一个定量的指标来衡量模型的整体性能，这就是AUC。

**AUC就是ROC曲线下方的面积。**

###### AUC的实际意义

AUC的取值范围在0到1之间，其背后有一个非常直观和重要的概率解释，这也是图片中**“AUC考虑的是样本预测的排序质量”**这句话的精髓所在：

> **AUC的数值，等于从所有正样本（真病人）中随机抽取一个，再从所有负样本（健康人）中随机抽取一个，这个正样本的预测得分高于这个负样本预测得分的概率。**

简单来说：

*   **AUC = 1：** 完美模型。它给所有真病人的打分，都高于所有健康人的打分。
*   **AUC = 0.8：** 意味着，随机抽取一个真病人和一个健康人，有80%的概率，这个真病人的得分会比健康人高。
*   **AUC = 0.5：** 随机猜测模型。模型对于真病人和健康人的打分是随机的，排序能力为零。
*   **AUC < 0.5：** 模型的效果比随机猜测还差，它倾向于把正样本排在负样本后面（比如把真病人普遍打低分，健康人普遍打高分）。

所以，**AUC衡量的不是模型预测的“准确度”，而是它对正负样本的“区分能力”或“排序能力”。** 一个高AUC的模型，能很好地将正样本排在负样本前面。

---

###### 结合教材图片内容解读

1.  **“若一个学习器的ROC曲线被另一个学习器的曲线完全‘包住’，则可断言后者的性能优于前者”**
    *   **解释：** 这意味着模型B（包住别人的）在任何FPR水平下，其TPR都比模型A（被包住的）要高。无论医生选择多么严格或宽松的诊断标准（阈值），B模型的效果总是更好。所以B模型完胜。

2.  **“若两个学习器的ROC曲线发生交叉，则难以一般性地断言两者孰优孰劣...此时如果一定要进行比较，则较为合理的判据是比较ROC曲线下的面积，即AUC”**
    *   **解释：** 交叉意味着在某些FPR区间，A模型更好（TPR更高）；在另一些FPR区间，B模型更好。这时就要看具体应用场景。比如，对于癌症筛查，我们宁愿误诊（高FPR）也不想漏诊（追求高TPR），可能会选择在右侧表现更好的模型。如果无法确定应用场景，就用AUC这个综合指标来评价，AUC大的模型整体区分能力更强。

3.  **“形式化地看，AUC考虑的是样本预测的排序质量”**
    *   **解释：** 这正是我上面强调的AUC的概率解释。它不关心具体的得分值，只关心正样本的得分是否普遍高于负样本的得分。`l_rank`公式计算的就是“排序损失”（一个正样本排在一个负样本之后的概率），所以`AUC = 1 - l_rank`。

###### 总结

| 特性         | ROC曲线                                                      | AUC                                                    |
| :----------- | :----------------------------------------------------------- | :----------------------------------------------------- |
| **形态**     | 一条二维曲线                                                 | 一个0到1的数值                                         |
| **核心意义** | 可视化地展示模型在**所有阈值**下的**性能权衡**（TPR vs. FPR） | **量化**模型的**整体区分能力**或**排序能力**           |
| **优点**     | 直观，能看到模型在不同决策偏好下的表现                       | 单一数值，便于模型间直接比较，且对样本类别不平衡不敏感 |
| **如何使用** | - 看曲线是否靠近左上角<br>- 对比不同模型的曲线（“包住”或“交叉”）<br>- 根据业务需求选择合适的阈值（操作点） | - AUC值越大，模型越好<br>- 作为模型优化的核心指标之一  |

希望这个详细的解释能帮助你透彻理解ROC曲线和AUC的实际意义！

绝佳的问题！这句对AUC的概率解释是理解其**核心价值**的关键。很多人知道AUC是曲线下的面积，但并不清楚这个面积代表的实际意义。

我们来用一个非常具体、直观的方式来拆解这句话。

---

###### 1. 创造一个“微型世界”

假设我们的AI模型已经对一小批人进行了肿瘤风险打分。这个世界里只有：

*   **3个真病人 (正样本)**
*   **4个健康人 (负样本)**

模型给出的得分如下（得分越高，模型认为越有可能是病人）：

| 身份         | 编号 | 模型得分 |
| :----------- | :--- | :------- |
| **病人 (P)** | P1   | **0.9**  |
| 健康人 (H)   | H1   | 0.7      |
| **病人 (P)** | P2   | **0.6**  |
| **病人 (P)** | P3   | **0.5**  |
| 健康人 (H)   | H2   | 0.4      |
| 健康人 (H)   | H3   | 0.2      |
| 健康人 (H)   | H4   | 0.1      |

**理想情况：**一个完美的模型，应该给所有病人的得分都高于所有健康人。
**现实情况：**如上表，模型犯了些错误。比如健康人H1的得分(0.7)就比病人P2(0.6)和P3(0.5)的得分要高。这说明模型的排序能力不是100%完美的。

AUC衡量的就是这种**排序的“完美程度”**。

---

###### 2. 玩一个“抽卡配对”游戏

现在，我们来严格按照那句话的定义玩一个游戏：

> “从所有正样本（病人）中**随机抽取一个**，再从所有负样本（健康人）中**随机抽取一个**，比较他们的得分。”

在我们的微型世界里，有多少种可能的配对组合呢？
*   病人有3个，健康人有4个。
*   所以总的配对组合数是 `3 × 4 = 12` 种。

我们把这12种组合全部列出来，看看在多少种组合里，**病人的得分 > 健康人的得分**。

| 病人 (得分)  | 健康人 (得分) | 比较结果 (病人得分 > 健康人得分?) |
| :----------- | :------------ | :-------------------------------- |
| P1 (**0.9**) | H1 (0.7)      | **是** (0.9 > 0.7)                |
| P1 (**0.9**) | H2 (0.4)      | **是** (0.9 > 0.4)                |
| P1 (**0.9**) | H3 (0.2)      | **是** (0.9 > 0.2)                |
| P1 (**0.9**) | H4 (0.1)      | **是** (0.9 > 0.1)                |
| ---          | ---           | ---                               |
| P2 (**0.6**) | H1 (0.7)      | 否 (0.6 < 0.7)                    |
| P2 (**0.6**) | H2 (0.4)      | **是** (0.6 > 0.4)                |
| P2 (**0.6**) | H3 (0.2)      | **是** (0.6 > 0.2)                |
| P2 (**0.6**) | H4 (0.1)      | **是** (0.6 > 0.1)                |
| ---          | ---           | ---                               |
| P3 (**0.5**) | H1 (0.7)      | 否 (0.5 < 0.7)                    |
| P3 (**0.5**) | H2 (0.4)      | **是** (0.5 > 0.4)                |
| P3 (**0.5**) | H3 (0.2)      | **是** (0.5 > 0.2)                |
| P3 (**0.5**) | H4 (0.1)      | **是** (0.5 > 0.1)                |

现在我们来数一下“**是**”的个数：`4 + 3 + 3 = 10` 个。

所以，在12次随机配对中，有10次我们会抽到“病人得分高于健康人得分”的情况。

这个概率就是 `10 / 12 ≈ 0.833`。

**这个0.833，就是这个模型在这个数据集上的AUC值！**

---

###### 3. 从“配对游戏”到“曲线下面积”

你可能会问：这个配对计数的概率，怎么就等于ROC曲线下方的面积了呢？

这背后的数学联系非常巧妙，可以这样理解：

绘制ROC曲线时，我们其实是在按得分从高到低排序，然后一个个地处理样本。
*   每遇到一个**病人（正样本）**，我们就在曲线上向上走一步。
*   每遇到一个**健康人（负样本）**，我们就在曲线上向右走一步。

现在，我们聚焦于任何一个**健康人（负样本）**，比如H1(0.7)。当我们在绘图时处理到它，准备向右走一步时，我们已经遇到了多少个病人？
在我们的例子里，得分比H1(0.7)高的病人只有P1(0.9)。也就是说，我们已经向上走过了一步（代表P1）。此时的“真正例率(TPR)”是 `1/3`。

当我们因为H1而向右走一步时，我们在ROC曲线下增加了一块微小的矩形面积。这个矩形的**高度**，恰好就是**得分比H1高的病人的比例（1/3）**。这个矩形的**宽度**是 `1/（健康人总数）`，即`1/4`。

把所有因为健康人（负样本）而产生的微小矩形面积加起来，总面积就等于：
`Σ [ (得分比这个健康人高的病人比例) × (1 / 健康人总数) ]`
这个公式经过推导，其结果**完全等价于**我们上面计算的配对概率 `10/12`。

---

###### 总结：为什么这个解释如此重要？

1.  **更直观：** “面积”是一个几何概念，而“概率”是一个统计概念。**“一个模型有多大概率能正确地把病人排在健康人前面”**，这个说法比“曲线下面积是0.83”更容易被业务人员理解。它直接反映了模型的核心任务——区分能力。

2.  **解释了AUC的本质：** AUC衡量的是**排序质量**，而不是绝对的预测值有多准。即使模型给所有病人的得分都在(0.4, 0.5)之间，给所有健康人的得分都在(0.1, 0.2)之间，这个模型的AUC依然是1.0（完美），因为它完美地将两类样本分开了。

3.  **解释了为何AUC对类别不平衡不敏感：** 在上面的配对游戏中，即使我们有3个病人和400个健康人，配对的总数会变成`3 × 400 = 1200`，但计算概率的方式完全一样。它关注的是两类样本之间的相对顺序，而不是某个类别的数量。这使得AUC在处理现实世界中常见的类别不平衡问题（如欺诈检测、罕见病诊断）时，比准确率等指标更为可靠。





### 代价敏感错误率与代价曲线

在现实任务中常会遇到这样的情况，不同类型的错误造成的后果不同，例如在医疗诊断中，错误地把患者诊断为健康人与错误地把健康人诊断为患者，看起来都是犯了“一次错误”，但后者的影响是增加了进一步检查的麻烦，前者的后果却可能是丧失了拯救生命的最佳时机；再如，门禁系统错误地把可通行人员拦在门外，将使得用户体验不佳，但错误地把陌生人放进门内，则会造成严重的安全事故。为权衡不同类型错误所造成的不同损失，可为错误赋予“非均等代价”(unequal cost)。

![二分类代价矩阵](https://cdn.jsdelivr.net/gh/Gongzihang6/Pictures@main/Medias/medias%2F2025%2F07%2Fimage-20250704102530899.png)

前面的性能度量大都隐式的假设了均等代价，并没有考虑不同错误会造成不同的后果，在非均等代价下，若将上图中的第0类作为正类，第1类作为反类，令D^+与D^-分别代表样例集D的正例子集和反例子集，则“代价敏感（cost-sensitive）错误率”为：
$$
E(f;D;cost)\frac{1}{m}\left(\sum_{x_i \in D^+} \mathbb{I}\left(f(x_i)\neq y_i \right)\times cost_{01}+\sum_{x_i \in D^-} \mathbb{I}\left(f(x_i)\neq y_i\right) \times cost_{10}\right)
$$
在非均等代价下，ROC 曲线不能直接反映出学习器的期望总体代价，而“代价曲线”(cost curve) 则可达到该目的。代价曲线图的横轴是取值为 [0, 1] 的正例概率代价

$$
P(+)cost = \frac{p \times cost_{01}}{p \times cost_{01} + (1 - p) \times cost_{10}},
$$

其中 $p$ 是样例为正例的概率；纵轴是取值为 [0, 1] 的归一化代价

$$
cost_{norm} = \frac{FNR \times p \times cost_{01} + FPR \times (1 - p) \times cost_{10}}{p \times cost_{01} + (1 - p) \times cost_{10}},
$$

其中 $FPR$ 是式(2.19)定义的假正例率，$FNR = 1 - TPR$ 是假反例率。代价曲线的绘制很简单：ROC 曲线上每一点对应了代价平面上的一条线段，设 ROC 曲线上点的坐标为 $(TPR, FPR)$，则可相应计算出 $FNR$，然后在代价平面上绘制一条从 $(0, FPR)$ 到 $(1, FNR)$ 的线段，线段下的面积即表示了该条件下的期望总体代价；如此将 ROC 曲线上的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价，如下图所示。

![代价曲线与期望总体代价](https://cdn.jsdelivr.net/gh/Gongzihang6/Pictures@main/Medias/medias%2F2025%2F07%2Fimage-20250704225910193.png)

















