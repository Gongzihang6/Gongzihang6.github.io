# 支持向量机（Support Vector Machine，SVM）

SVM是特征空间上的间隔最大的线性二分类器，其学习策略是间隔最大化，最终可转化为一个凸二次规划问题求解，因此SVM的解是全局最优解。

SVM在解决小样本、非线性以及高维问题中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中；

## 间隔与支持向量

给定训练样本集$D={(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)}$，$y_i\in\{-1,+1\}$，分类学习最基本的想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开，但是能将训练样本分开的超平面可能有很多

<img src="F:\software\Typora\images\image-20250622154146030.png" alt="存在多个超平面可以将训练样本分开" style="zoom:80%;" />

直观上看，我们应该选择位于两类训练样本“正中间”的划分超平面，也就是图中的粗线表示的超平面，因为该超平面对训练样本的局部扰动的容忍性最好，产生的分类结果是最鲁棒的，对未见示例的泛化能力最强；

在样本空间中，划分超平面可通过如下线性方程来描述：
$$
w^Tx+b=0
$$
其中$w=(w_1,w_2,\dots,w_d)^T$为法向量，决定了超平面的方向，$b$为位移项，决定了超平面与原点之间的距离，划分超平面由法向量$w$和位移$b$确定；

样本空间中任意点x到超平面(w,b)的距离可以写为：
$$
r=\frac{|w^Tx+b|}{||W||}
$$
假设超平面(w,b)能将训练样本正确分类，即对于(x_i,y_i)\in D，若y_i=+1，则有w^Tx_i+b>0；若y_i=-1，则有w^Tx_i+b<0，即：
$$
\begin{cases}
w^Tx_i+b \ge +1, \quad y_i=+1;	\\
w^Tx_i+b \le -1, \quad y_i=-1;
\end{cases}
$$








































