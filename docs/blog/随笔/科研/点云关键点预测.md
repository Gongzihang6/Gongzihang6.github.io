目前主要尝试两种方式，一种是直接基于三维点云的方式，另一种是基于彩色图像的方式；

## 基于三维点云的关键点预测
主要可以使用的是点云类的模型，比如PointNet，PointNet++，PointTransformer、PointNext等等一系列模型，这些模型可能是用于语义分割的，也就对点云进行逐点分类，我们的关键点检测任务，也可以视为一个逐点分类的任务，只不过真实点很少，每个类别的真实点只有一个；或者也可以更改模型的输出头，比如将关键点检测视为关键点坐标回归任务，训练模型直接输出关键点坐标；或者采用热力图式的软标签，训练模型输出点云中每个点隶属于每个关键点的概率值，然后选择使用argmax或者soft-argmax的方式计算关键点坐标；

以目前尝试的结果来看，由于关键点数量占整体点云数量比例极低，采用热力图式的软标签方式很难训练出来，因为样本的软标签标签 **N*k **的矩阵中（N为点云中点的个数，k为关键点个数），k列中只有少数点是概率值比较大的，其余大部分都是0，也就是说这是一个稀疏矩阵，模型很难去学习到每一列的分布，而是倾向于偷懒，干脆让N*k的矩阵的每一个元素都是1/Nk，导致最后预测到的点都重叠在质心位置；只有使用预测关键点和真实关键点之间的距离误差，才能强迫让预测的关键点坐标向真实关键点坐标位置靠近；

### 基于改进的PointNet++的关键点预测
仿照论文([https://linkinghub.elsevier.com/retrieve/pii/S0168169925003916](https://linkinghub.elsevier.com/retrieve/pii/S0168169925003916))A geodesic distance regression-based semantic keypoints detection method for pig point clouds and body size measurement中的方法，论文中标注了8个关键点，如下图所示：

![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763030919626-579a5bc4-2892-434f-bbc2-1189e82e0b34.png)

但是其中用于定位胸围和臀围的关键点比较难以定位，比较模糊，所以我们选择在背部使用6个关键点来定位各项体尺参数的测量位置，如下图所示，可以仅使用背部的6个关键点来定位，同样可以达到比较好的效果。

==============================================================

           体尺测量结果分析与误差评估

==============================================================

参数    真实值(mm)      估测值(mm)      绝对误差    相对误差(%)

------------------------------------------------------------------

体长：  1260.00         1269.70         9.70        0.77        %

体高：  820.00          794.68          -25.32      -3.09       %

体宽：  370.00          401.37          31.37       8.48        %

胸围：  1380.00         1421.05         41.05       2.97        %

腰围：  1660.00         1613.62         -46.38      -2.79       %

臀围：  1450.00         1443.74         -6.26       -0.43       %

==============================================================

           总体误差评估 (6项平均)

--------------------------------------------------------------

平均绝对误差 (MAE):           26.68      mm

平均绝对百分比误差 (MAPE):    3.09       %

==============================================================

![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763031226651-cb813c32-a19e-4783-88ab-722583517e03.png)

![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763031274342-9087f8ce-58d6-4fc1-9763-52f371d238a5.png)

论文中的模型结构大致如下：

![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763031374956-1b58410f-432a-47ee-bea0-5555ce0c5d3e.png)

核心改动在于，原始的PointNet++的SA层中，在FPS下采样之后，会进行球状邻域查询，在下采样的点的周围选取最近的K个点，然后进行最大池化，再经过MLP来提取特征；论文中对这里的改进是使用K个邻居点到采样点的坐标差和特征差作为输入，使用MLP学习这K个邻居点的特征的相对重要关系，结构如下图所示，然后对这K个邻居点的特征进行加权求和，得到最终的特征输出，作为这个球状邻域的特征表示，进入下一层抽象。

![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763032246296-701f9909-938d-4842-aa69-aabd94c1b156.png)

#### WFA改进结果
这种方法WFA加权下实验结果如下：

![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763033519235-b28954cd-81ba-4b84-8200-42e1aeb00325.png)

![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763033549369-3744dac6-f79a-42ab-b906-feb19e02db54.png)

![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763033569982-a1163f9d-8c3e-429b-8c0a-3ffbea7ac841.png)

对应的体尺估测结果如下：

![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090081940-c9fba170-d543-4eac-a078-e4b5d4006f23.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090081895-5313cc2f-70b4-4c95-adc6-1d5fbe9a3081.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090081906-496a4b93-969e-402a-bcc1-7192a1fe8099.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090081904-1d1a1e04-45af-43a6-bae2-86fd70719897.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090082047-d689cce3-4f7b-4a8e-bb54-94e2cb200a61.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090082270-5f52f123-5151-4754-82c5-f3f6c1719043.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090082293-4a7eeca2-cdeb-4671-b161-3b7838104c2e.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090082470-8b3f6251-ca74-48c9-aada-1ae89424ec86.png)



#### 自注意力改进结果
这里我尝试了使用自注意力机制来改进，既然是计算这K个邻居点特征的相对重要性，注意力机制天生就是用来计算重要性的，使用自注意力机制来计算这K个邻居点的权重，然后加权求和输出；

实验结果如下：

![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763033684355-314c1fbb-1b2c-47db-a53a-a3d4720d5d80.png)

![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763033701191-456c690b-d382-46f0-ad0e-34e14b86de7d.png)

![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763033712923-d45176f7-477f-4bf3-b4a9-1a879946fc7e.png)

对应的体尺估测结果如下：

![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090603845-2056eb8c-764b-4556-a14e-d8e8546fc781.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090604098-bfc1b164-b9b4-4d64-92a4-26c5bd6450f0.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090604076-c809b5eb-aeaa-486b-87b6-b112925cf922.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090604151-e5ad13ac-b7e0-4651-934a-7ffb65cb2172.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090604210-72f699c2-53e1-4093-bd04-2b214c0cba97.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090604220-f1be2878-6646-4476-96ed-148a007fba99.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090604521-c98d1df0-1b33-4e29-b57d-152ab0ff65a4.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090604553-eb6bbd9a-c651-46d6-a13b-91df38493d8b.png)

#### 背部点云实验结果
如果只使用背部点云来预测关键点（因为关键点都在背部），实验结果如下：

![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763033854246-36f4cc92-4c09-42e4-bda2-f4436fd1f7b5.png)

![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763033869267-2f3c112f-a1ee-4b20-9906-8ec23abea852.png)

![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763033881387-7be20a97-94eb-4b1f-bd3a-b89708336dfe.png)



对应的体尺估测结果如下：

![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090631217-d2fafd8e-1995-4966-a4f3-cf294079dc7c.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090631204-eab8773a-4c6d-4055-96c9-de70a22f4fde.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090631205-64420668-e618-4cac-a0d0-51fe3517d7ee.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090631211-f628919b-7861-4c9d-b6f3-a1881cfaa9bc.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090631352-7e97d0bc-2bf0-45e0-bb76-fa0edaa959b5.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090631666-4658a31e-3f5f-4fb7-a17e-67ec7c0293a3.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090631632-8d90c4ab-7315-4db7-96a3-9de33edf285f.png)![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763090631780-5db8c1fb-90a5-4bcc-8089-c6028233f2e4.png)

## 基于彩色图像的关键点预测
由于获取了彩色图像和彩色点云，彩色点云一般都是有组织有结构的点云，且点的数量和彩色图像中的像素点个数一一对应，具体来说，对于H*W的彩色图像，有H*W个像素点，对应的彩色点云就有H*W个三维点，彩色图像中的像素坐标为（u，v）的关键点，对应到彩色点云中就是索引为u*(W-1)+v的三维点，因此可以通过检测背部彩色图像上的关键点，然后通过上述的映射方式对应到点云中的三维点坐标；

### 基于YOLO11-Pose的关键点预测
这里对 Ultralytics YOLO11n-Pose 的网络结构进行深入、细致的解析，覆盖模型的骨干、颈部、头部各模块的具体网络结构、它们之间的连接关系、关键超参数，以及从一张输入图像与其标签进入模型到输出的完整流转过程（含张量形状变化与解码细节）。

#### 总览
+ 任务类型：`pose`（关键点检测，支持目标框与关键点的联合预测）
+ 参考配置：`ultralytics/cfg/models/11/yolo11-pose.yaml:1-51`
+ 主体结构：Backbone（下采样+特征抽取）→ Neck/FPN（上采样+特征融合）→ Head（检测头+关键点头）
+ 多尺度输出：`P3`(1/8)、`P4`(1/16)、`P5`(1/32)
+ 规模控制（Compound Scaling）：`scales.n = [depth=0.50, width=0.25, max_channels=1024]`（`yolo11-pose.yaml:8-14`）
+ 自定义关键点：`kpt_shape: [6, 2]`（示例为猪背部6关键点，二维坐标），`yolo11-pose.yaml:7`

#### 模型结构与连接关系
##### Backbone（骨干网络）
源定义：`ultralytics/cfg/models/11/yolo11-pose.yaml:16-30`

+ `Conv(64, k=3, s=2)`：输入图像下采样至 `P1/2`
+ `Conv(128, k=3, s=2)`：进一步下采样至 `P2/4`
+ `C3k2(256, n=2, c3k=False, e=0.25)`：CSP/C2f 变体，含瓶颈结构堆叠（深度按 `depth_multiple` 缩放）
+ `Conv(256, k=3, s=2)`：下采样到 `P3/8`
+ `C3k2(512, n=2, c3k=False, e=0.25)`：更深的特征抽取
+ `Conv(512, k=3, s=2)`：下采样到 `P4/16`
+ `C3k2(512, n=2, c3k=True)`：使用 `C3k` 子模块，核更可控
+ `Conv(1024, k=3, s=2)`：下采样到 `P5/32`
+ `C3k2(1024, n=2, c3k=True)`：顶层深特征
+ `SPPF(1024, k=5)`：金字塔池化加速版，扩大感受野

模块实现细节：

+ `Conv`：标准 `Conv-BN-Activation`，默认激活 `SiLU`，定义见 `ultralytics/nn/modules/conv.py:37-49, 102-116`
+ `C3k2`：基于 `C2f` 的快速 CSP 结构，内部使用 `Bottleneck` 或 `C3k` 堆叠（按 `n` 次），定义见 `ultralytics/nn/modules/block.py:1063-1084`
+ `C3k`：可调核大小的 CSP 瓶颈模块，定义见 `ultralytics/nn/modules/block.py:1086-1105`
+ `SPPF`：快速空间金字塔池化，定义见 `ultralytics/nn/modules/block.py:206-232`

##### Neck/FPN（特征金字塔融合）
源定义：`ultralytics/cfg/models/11/yolo11-pose.yaml:34-50`

+ 通过 `nn.Upsample(scale=2)` 将 `P5` 上采样，与 `P4` 级特征 `Concat`，经 `C3k2(512)` 融合 → 得到新的 `P4`
+ 再将融合后的 `P4` 上采样，与 `P3` 级特征 `Concat`，经 `C3k2(256)` 融合 → 得到新的 `P3`（Small）
+ 将新的 `P3` 下采样与上一层头部 `P4` `Concat`，经 `C3k2(512)` → 得到 `P4`（Medium）
+ 将新的 `P4` 下采样与 `SPPF` 后的 `P5` `Concat`，经 `C3k2(1024, True)` → 得到 `P5`（Large）
+ `Concat` 定义见 `ultralytics/nn/modules/conv.py:614-641`，上采样使用 `torch.nn.Upsample`

##### Head（检测头 + 关键点头）
最后一层：`Pose(P3,P4,P5)`，定义见 `ultralytics/nn/modules/head.py:319-385`

+ `Detect` 分支：每个尺度上由 `cv2`（框回归）与 `cv3`（分类）构成，整体推理融合见 `Detect._inference`（`ultralytics/nn/modules/head.py:149-168`）
    - DFL 分布回归（`reg_max=16`）将 `dist(l,t,r,b)` 解码为框坐标，见 `ultralytics/utils/tal.py:367-376`
    - 锚点/步长生成：`make_anchors`（`ultralytics/utils/tal.py:352-365`）
+ `Pose` 关键点分支：为每尺度增加 `cv4` 序列（`Conv → Conv → Conv1x1(nk)`），将每尺度特征映射到 `nk = num_kpts * dims` 通道，定义见 `ultralytics/nn/modules/head.py:340-355`
    - 输出拼接形状：`(bs, nk, Σ(h_i*w_i))`，随后通过 `kpts_decode` 解码到像素坐标（或导出模式下嵌入可见性），见 `ultralytics/nn/modules/head.py:365-385`
    - 解码公式（非导出模式）：
        * `x = (pred_x * 2 + (anchors_x - 0.5)) * stride`
        * `y = (pred_y * 2 + (anchors_y - 0.5)) * stride`

#### 模型缩放（Compound Scaling）与有效通道数
源代码：`ultralytics/nn/tasks.py:1502-1510, 1577-1660`

+ `scales.n = [depth=0.50, width=0.25, max_channels=1024]`
+ 宽度缩放：模块声明的通道 `c2` 会按 `make_divisible(min(c2, max_channels) * width, 8)` 缩减
    - 例：`64 → 16`、`128 → 32`、`256 → 64`、`512 → 128`、`1024 → 256`
+ 深度缩放：`n' = max(round(n * depth), 1)`，如 `n=2` 在 `n` 规模下有效重复为 `1`
+ `Concat` 输出通道等于来自各分支通道之和（已按宽度缩放），见 `ultralytics/nn/tasks.py:1624-1626`

#### 超参数（训练/推理关键项）
源配置：`ultralytics/cfg/default.yaml:93-125`

+ 优化器与基础学习率：`optimizer`、`lr0`、`lrf`、`momentum`、`weight_decay`
+ 损失权重：
    - `box`: 边框损失增益
    - `cls`: 分类损失增益
    - `dfl`: 分布焦点损失增益
    - `pose`: 关键点回归整体增益（pose任务）
    - `kobj`: 关键点对象性损失增益（pose任务）
+ 训练增强：`mosaic`、`mixup`、`hsv_*`、`flipud`、`fliplr` 等（如需禁用，置 `0.0`）
+ 推理设置：`conf`、`iou`、`max_det`、`augment`、`show_*`

#### 标签格式与数据读取
校验解析：`ultralytics/data/utils.py:200-246`；数据集类：`ultralytics/data/dataset.py:74-90, 206-235`

+ 单行标签（关键点任务）列数：`5 + nkpt * ndim`
    - `class_id, cx, cy, w, h, kpt1_x, kpt1_y, kpt2_x, kpt2_y, ...`（归一化坐标，范围约 `[-0.01, 1.01]`）
    - 若 `ndim=3`，每个关键点额外有可见性维度（`v∈{0,1}`），解析时会拼接 mask（`dataset.py:240-245`）
+ 读取后进入 `YOLODataset`，按任务类型 `pose` 进行 `use_keypoints=True`，并在 `Format` 变换中保留关键点（`dataset.py:223-235`）

#### 前向传播数据流与形状变化（以输入 `B,3,640,640` 为例，scale=`n`）
注：下列通道为“有效通道”（宽度缩放后），`h×w` 为空间分辨率，`Σ(h_i*w_i)` 为多尺度网格总和。

1. Backbone 下采样与特征抽取
+ `Conv(64,s=2)` → `B,16,320,320`
+ `Conv(128,s=2)` → `B,32,160,160`
+ `C3k2(256,n≈1)` → `B,64,160,160`
+ `Conv(256,s=2)` → `B,64,80,80`（`P3`）
+ `C3k2(512,n≈1)` → `B,128,80,80`
+ `Conv(512,s=2)` → `B,128,40,40`（`P4`）
+ `C3k2(512,n≈1,c3k=True)` → `B,128,40,40`
+ `Conv(1024,s=2)` → `B,256,20,20`（`P5`）
+ `C3k2(1024,n≈1,c3k=True)` → `B,256,20,20`
+ `SPPF(1024)` → `B,256,20,20`
2. Neck/FPN 融合（自顶向下 + 自底向上）
+ `Upsample(P5)` + `Concat(P4)` + `C3k2(512)` → 新 `P4`：`B,128,40,40`
+ `Upsample(P4)` + `Concat(P3)` + `C3k2(256)` → 新 `P3`：`B,64,80,80`
+ `Conv(s=2)` + `Concat(P4)` + `C3k2(512)` → `P4`：`B,128,40,40`
+ `Conv(s=2)` + `Concat(P5)` + `C3k2(1024,True)` → `P5`：`B,256,20,20`
3. Head：检测与关键点预测
+ 多尺度特征列表：`[P3: B,64,80,80; P4: B,128,40,40; P5: B,256,20,20]`
+ `Detect` 分支：每尺度产生 `reg_max*4 + nc` 通道（框分布与分类），在推理合并后形状为：
    - `box`: `(B, Σ(h_i*w_i), 4)`（解码后）
    - `cls`: `(B, Σ(h_i*w_i), nc)`（Sigmoid）
+ `Pose` 分支 `cv4`：每尺度卷积到 `nk = kpt_num * dims` 通道，例如 `kpt_num=6, dims=2 → nk=12`
    - 拼接后 `kpt_raw`: `(B, 12, 80*80 + 40*40 + 20*20) = (B,12,8400)`
    - 解码后 `kpt_xy`: `(B, 12, 8400)`，两个通道分别是 `x`、`y` 像素坐标（若 `dims=3` 则携带可见性 `v`），解码见 `ultralytics/nn/modules/head.py:365-385`
4. 锚点与步长
+ 锚点按网格中心生成：`make_anchors`，见 `ultralytics/utils/tal.py:352-365`
+ 步长对应尺度：`P3→8`、`P4→16`、`P5→32`，在 `Detect._inference` 中维护，见 `ultralytics/nn/modules/head.py:161-167`

#### 训练过程与损失
损失实现：`ultralytics/utils/loss.py`

+ 总体由 `v8PoseLoss` 管理（定义见 `ultralytics/utils/loss.py:484-497`），继承检测损失 `v8DetectionLoss`（`ultralytics/utils/loss.py:194-303`）
+ 检测部分（框+分类+DFL）使用 TAL（TaskAlignedAssigner）进行正样本分配
+ 关键点部分：`KeypointLoss`（`ultralytics/utils/loss.py:175-191`）
    - 对关键点像素坐标采用欧氏距离并归一化为 OKS 风格度量，结合可见性 mask 与实例面积
    - 损失增益由 `default.yaml` 中 `pose` 与 `kobj` 控制
+ Pose 模型类负责将 `kpt_shape` 带入与初始化损失器：`ultralytics/nn/tasks.py:570-589`

#### 推理输出与关键点访问
+ 推理结果类：`ultralytics/engine/results.py`，关键点对象 `Keypoints` 提供：
    - `xy`（像素坐标）：`r.keypoints.xy`，访问见 `engine/results.py` 中属性定义（该文件包含 `Keypoints`）
    - `xyn`（归一化坐标）等
+ 实例示意：
    - `r.boxes.xyxy`: 预测框像素坐标（`B×N×4`）
    - `r.keypoints.xy`: 预测关键点像素坐标（`B×N×K×2`）

#### 关键模块的网络结构详细说明
##### Conv（带 BN 与激活）
+ 结构：`Conv2d → BatchNorm2d → SiLU`
+ 作用：通道变换与空间抽取，可通过 `stride` 控制下采样
+ 定义：`ultralytics/nn/modules/conv.py:37-49`

##### C3k2（快速CSP/C2f变体）
+ 结构：基于 `C2f` 的双分支输入后聚合，内部堆叠 `Bottleneck` 或 `C3k` 模块（次数由 `n`）
+ 关键参数：`n`（重复次数）、`c3k`（是否用 C3k 子结构）、`e`（通道膨胀比）、`shortcut`、`groups`
+ 定义：`ultralytics/nn/modules/block.py:1063-1084`

##### C3k（可调核的 CSP 瓶颈）
+ 结构：`1x1` 降维后，重复 `Bottleneck(k=k)` 堆叠，强调多尺度卷积核
+ 定义：`ultralytics/nn/modules/block.py:1086-1105`

##### SPPF（空间金字塔池化-快速）
+ 结构：`MaxPool(k) × 3` 级联 + 连接 + `Conv`
+ 作用：增强高层特征的感受野与上下文融合
+ 定义：`ultralytics/nn/modules/block.py:206-232`

##### Concat（特征拼接）
+ 结构：按维度拼接张量，通常为通道维（`dim=1`）
+ 作用：融合来自不同尺度或来源的特征图
+ 定义：`ultralytics/nn/modules/conv.py:614-641`

##### Detect / Pose 头
+ `Detect`：框分布与分类两分支，推理时统一解码；定义与推理：`ultralytics/nn/modules/head.py:26-125, 149-168`
+ `Pose`：在 `Detect` 之上增加关键点分支 `cv4`，并提供解码方法；定义：`ultralytics/nn/modules/head.py:319-385`

#### 从输入图像与标签到输出的完整流转（详细）
1. 输入与预处理
+ 输入图：`B,3,640,640`，在数据集管线中可进行 LetterBox、归一化等（`ultralytics/data/dataset.py:206-235`）
+ 标签：每目标一行，`[cls, cx, cy, w, h, kpt1_x, kpt1_y, ..., kptK_y(, v?) ]`；检查与解析见 `ultralytics/data/utils.py:200-246`
2. 骨干特征抽取
+ 连续 `Conv` 下采样与 `C3k2` 特征堆叠，得到多尺度主干特征 `P3/P4/P5`
+ `SPPF` 在顶层加强上下文表达
3. FPN 融合
+ 自顶向下的上采样与与主干侧支 `Concat` → `C3k2` 融合，形成语义丰富且空间分辨率较高的 `P3/P4`
+ 自底向上的再下采样与拼接 → `C3k2` 融合，形成最终 `P4/P5`
4. 检测与关键点预测
+ `Detect`：每尺度经 `cv2/cv3` 产生框分布与分类分数，推理时拼接并解码为 `(B, Σ, 4+nc)`
+ `Pose`：每尺度经 `cv4` 产生 `nk` 通道，拼接为 `(B, nk, Σ)`，随后按锚点与步长解码到像素坐标
5. 解码与输出装配
+ 锚点：每尺度网格中心点 `(sx,sy)`；步长为尺度对应的下采样率（8、16、32）
+ 框解码：`dist2bbox`（`ultralytics/utils/tal.py:367-376`）得到 `xywh/xyxy`
+ 关键点解码：`kpts_decode`（`ultralytics/nn/modules/head.py:365-385`）线性变换到像素坐标，并处理可见性维度（如有）
+ 输出结果对象包含 `boxes` 与 `keypoints`，关键点像素坐标访问 `r.keypoints.xy`
6. 损失计算与反向传播（训练）
+ 检测损失：`box + cls + dfl`，由 TAL 分配正样本与目标，见 `ultralytics/utils/loss.py:194-303`
+ 关键点损失：`KeypointLoss`，使用欧氏距离与 OKS 风格归一化，见 `ultralytics/utils/loss.py:175-191`
+ 损失权重：`default.yaml` 中的 `box/cls/dfl/pose/kobj` 增益项（`ultralytics/cfg/default.yaml:101-106`）

#### 形状与数量的综合示例（以 6 个关键点、dims=2 为例）
+ 总网格数：`Σ = 80×80 + 40×40 + 20×20 = 8400`
+ 关键点通道 `nk = 6×2 = 12`
+ `kpt_raw`（拼接前）：每尺度为 `(B,12,h_i×w_i)`；拼接后为 `(B,12,8400)`
+ 解码后关键点仍为 `(B,12,8400)`，但数值从“偏移量”映射到“像素坐标”
+ 框与分类分支在推理合并后为 `(B,8400,4+nc)`，其中 `nc=1`（示例为单类别）

#### 关键点可见性（如使用 dims=3）
+ 若 `kpt_shape=[K,3]`，则每关键点包含 `(x,y,v)`：`v∈{0,1}` 或连续概率，训练 Label 解析会补充 mask（`ultralytics/data/utils.py:239-245`）
+ `kpts_decode` 会对可见性维度做 `sigmoid`（平台兼容处理），并与坐标一起打包输出（`ultralytics/nn/modules/head.py:369-384`）

#### 与 YOLOv8-Pose 的差异要点（简述）
+ YOLO11 使用 `C3k2` 与更丰富的头部/分支设计；YOLOv8 常用 `C2f` 与不同的头部配置（参见 `ultralytics/cfg/models/v8/yolov8-pose.yaml:1-50`）
+ YOLO11 在 `Detect` 的分类分支实现上区分 `legacy` 与新式 `DWConv+Conv` 组合（`ultralytics/nn/modules/head.py:96-107`）

#### 配置项回顾与实践建议
+ 若自定义 `kpt_shape` 与类别数 `nc`，请在模型 YAML 与数据集 YAML 保持一致；模型构建会根据数据集传入值覆盖 YAML（`ultralytics/nn/tasks.py:570-589`）
+ 训练增强的关闭需在训练参数中将所有相关概率置为 `0.0`（如 `mosaic/mixup/hsv/flip` 等），参考 `ultralytics/cfg/default.yaml`
+ 推理访问关键点请使用 `Results.keypoints.xy`（像素坐标），方便与原图对齐进行误差度量与可视化

---

以上内容对 YOLO11n-Pose 的结构、连接、超参数与数据流进行了逐层、逐项的详细解析，并给出了关键代码位置以供交叉验证。若需要针对你当前的 6 点猪背任务进一步导出特定算子形状或某一层的权重统计，也可以在此文基础上继续扩展。



使用100张图片的小数据集实验效果如下：

![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763089883180-ca81f137-9b2e-4b9d-afb2-63bca97f6d9b.png)

![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763089908116-2e0fe0d8-e7d9-43ce-91df-8dc71841cc4c.png)



![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763089970145-4da2b78c-7246-49cd-a400-951d3af39b80.png)



![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763089984060-70aa85db-245e-4695-898c-96bd53d39b92.png)

![](https://cdn.nlark.com/yuque/0/2025/png/47982009/1763089995579-f08d9fe9-189e-47de-8369-9ddda7118490.png)







