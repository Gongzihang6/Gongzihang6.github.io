## 牛顿法

牛顿法（Newton method）和拟牛顿法（quasi-Newton method）是求解无约束最优化问题的常用方法，有收敛速度快的优点。牛顿法是迭代算法，每一步需要求解目标函数的海塞矩阵的逆矩阵，计算比较复杂。拟牛顿法通过正定矩阵近似 **海塞矩阵的逆矩阵** 或 **海塞矩阵**，简化了这一计算过程。

考虑 **无约束** 最优化问题

$$
\min_{x \in \mathbb{R}^n} f(x) \tag{B.1}
$$

其中 $x^*$ 为目标函数的极小点。

假设 $f(x)$ 具有二阶连续偏导数，若第 $k$ 次迭代值为 $x^{(k)}$，则可将 $f(x)$ 在 $x^{(k)}$ 附近进行二阶泰勒展开：

$$
f(x) = f(x^{(k)}) + g_k^T (x - x^{(k)}) + \frac{1}{2}(x - x^{(k)})^T H(x^{(k)})(x - x^{(k)}) \tag{B.2}
$$

这里，式 B.2 是原始函数 $f(x)$ 在点 $x^{(k)}$ 附近的 **近似**，$g_k = g(x^{(k)}) = \nabla f(x^{(k)})$ 是 $f(x)$ 的梯度向量在点 $x^{(k)}$ 的值，$H(x^{(k)})$ 是 $f(x)$ 的黑塞矩阵（Hessian matrix）

$$
H(x) = \left [ \frac{\partial^2 f}{\partial x_i \partial x_j} \right]_{n \times n} \tag{B.3}
$$

在点 $x^{(k)}$ 的值。函数 $f(x)$ 有极值的必要条件是在极值点处一阶导数为 0，即梯度向量为 0。特别是当 $H(x^{(k)})$ 是正定矩阵时，函数 $f(x)$ 的极值为极小值。

牛顿法利用极小点的必要条件

$$
\nabla f(x) = 0 \tag{B.4}
$$

每次迭代中从点x^{(k)}开始，求目标函数的极小点（这里指的是目标函数在x^{(k)}附近的近似B.2的极小点，也就是x^{(k+1)}），作为第k+1次迭代值x^{(k+1)}，具体的，假设x^{(k+1)}满足：
$$
\nabla f(x^{(k+1)})=0 \tag{B.5}
$$
由式B.2，对x求导，得到目标函数的近似B.2的一阶导数，得到
$$
\nabla f(x)=g_k+H_k(x-x^{(k)}) \tag{B.6}
$$
其中H_k=H(x^{(k)})，即f(x)在点x^{(k)}处的海塞矩阵值，令上面的一阶导数在x^{(k+1)}处为0，得到：
$$
g_k+H_k(x^{(k+1)}-x^{(k)})=0	\tag{B.7}
$$
因此，

$$
x^{(k+1)} = x^{(k)} - H_k^{-1} g_k \tag{B.8}
$$

或者

$$
x^{(k+1)} = x^{(k)} + p_k \tag{B.9}
$$

其中，

$$
H_k p_k = -g_k \tag{B.10}
$$

用式 (B.8) 作为迭代公式的算法就是牛顿法。

算法 B.1（牛顿法）

输入：目标函数 $f(x)$，梯度 $g(x) = \nabla f(x)$，黑塞矩阵 $H(x)$，精度要求 $\varepsilon$；
输出：$f(x)$ 的极小点 $x^*$。

(1) 取初始点 $x^{(0)}$，置 $k = 0$。

(2) 计算 $g_k = g(x^{(k)})$。

(3) 若 $\|g_k\| < \varepsilon$，则停止计算，得近似解 $x^* = x^{(k)}$。

(4) 计算 $H_k = H(x^{(k)})$，并求 $p_k$
$$
H_k p_k = -g_k
$$

(5) 置 $x^{(k+1)} = x^{(k)} + p_k$。


## 拟牛顿法